{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57d11120-3ade-460f-a8e9-3ea5e297baed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keystroke dataset loaded: 19996 rows, features=['dwell_time', 'flight_time', 'up_down_time', 'session_duration', 'rhythm']\n",
      "\n",
      "=== SUMMARY ===============================\n",
      "Data type: keystroke\n",
      "Samples: 19996 | Base window: 800 | Roll window: 100 | Adapt window: 500\n",
      "Detectors: ['ADWIN_PC', 'PH_PC', 'KSWIN_PC', 'DDM_ERR', 'PCDM_PC', 'ADWIN_OCSVM', 'ADWIN_ISO']\n",
      "Consensus K: 3 | Match tolerance (DDA): ±25 samples\n",
      "------------------------------------------\n",
      "Total consensus drift points: 0\n",
      "------------------------------------------\n",
      "Per-detector detections:\n",
      "  ADWIN_PC    : 0 detections  |  DDA vs consensus: 0.000\n",
      "  PH_PC       : 0 detections  |  DDA vs consensus: 0.000\n",
      "  KSWIN_PC    : 0 detections  |  DDA vs consensus: 0.000\n",
      "  DDM_ERR     : 18760 detections  |  DDA vs consensus: 0.000\n",
      "  PCDM_PC     : 275 detections  |  DDA vs consensus: 0.000\n",
      "  ADWIN_OCSVM : 0 detections  |  DDA vs consensus: 0.000\n",
      "  ADWIN_ISO   : 0 detections  |  DDA vs consensus: 0.000\n",
      "------------------------------------------\n",
      "No AL measured (no consensus drifts or no recovery observed).\n",
      "==========================================\n",
      "\n",
      "Mouse dataset loaded: 252397 rows, features=['speed', 'distance']\n",
      "\n",
      "=== SUMMARY ===============================\n",
      "Data type: mouse\n",
      "Samples: 252397 | Base window: 1500 | Roll window: 200 | Adapt window: 800\n",
      "Detectors: ['ADWIN_PC', 'PH_PC', 'KSWIN_PC', 'DDM_ERR', 'PCDM_PC', 'ADWIN_OCSVM', 'ADWIN_ISO']\n",
      "Consensus K: 3 | Match tolerance (DDA): ±30 samples\n",
      "------------------------------------------\n",
      "Total consensus drift points: 0\n",
      "------------------------------------------\n",
      "Per-detector detections:\n",
      "  ADWIN_PC    : 0 detections  |  DDA vs consensus: 0.000\n",
      "  PH_PC       : 0 detections  |  DDA vs consensus: 0.000\n",
      "  KSWIN_PC    : 0 detections  |  DDA vs consensus: 0.000\n",
      "  DDM_ERR     : 0 detections  |  DDA vs consensus: 0.000\n",
      "  PCDM_PC     : 6035 detections  |  DDA vs consensus: 0.000\n",
      "  ADWIN_OCSVM : 0 detections  |  DDA vs consensus: 0.000\n",
      "  ADWIN_ISO   : 0 detections  |  DDA vs consensus: 0.000\n",
      "------------------------------------------\n",
      "No AL measured (no consensus drifts or no recovery observed).\n",
      "==========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from river.drift import ADWIN, PageHinkley, KSWIN\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -----------------------------\n",
    "# Lightweight custom detectors\n",
    "# -----------------------------\n",
    "class DDM:\n",
    "    \"\"\"Drift Detection Method on a 0/1 error stream\"\"\"\n",
    "    def __init__(self, min_num_instances=30, warning_level=2.0, drift_level=3.0):\n",
    "        self.min_num_instances = min_num_instances\n",
    "        self.warning_level = warning_level\n",
    "        self.drift_level = drift_level\n",
    "        self.mean = 0.0\n",
    "        self.var = 0.0\n",
    "        self.n = 0\n",
    "        self.mean_min = float('inf')\n",
    "        self.std_min = float('inf')\n",
    "\n",
    "    def add_element(self, error):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = error\n",
    "            self.var = 0.0\n",
    "        else:\n",
    "            # Welford update\n",
    "            prev_mean = self.mean\n",
    "            self.mean += (error - prev_mean) / self.n\n",
    "            self.var += (error - prev_mean) * (error - self.mean)\n",
    "\n",
    "        if self.n < 2:\n",
    "            std = 0.0\n",
    "        else:\n",
    "            std = np.sqrt(self.var / (self.n - 1))\n",
    "\n",
    "        if self.n >= self.min_num_instances:\n",
    "            # drift?\n",
    "            if self.mean + std > self.mean_min + self.drift_level * self.std_min:\n",
    "                return True\n",
    "            # update best-so-far\n",
    "            if self.mean + std < self.mean_min + self.warning_level * self.std_min:\n",
    "                self.mean_min = min(self.mean_min, self.mean)\n",
    "                self.std_min = min(self.std_min, std)\n",
    "        return False\n",
    "\n",
    "\n",
    "class PCDM:\n",
    "    \"\"\"Very small permutation test on a single stream value (e.g., PC1)\"\"\"\n",
    "    def __init__(self, window_size=50, n_permutations=50, alpha=0.01):\n",
    "        self.window_size = window_size\n",
    "        self.n_permutations = n_permutations\n",
    "        self.alpha = alpha\n",
    "        self.ref = []\n",
    "        self.cur = []\n",
    "\n",
    "    def add_element(self, v):\n",
    "        self.cur.append(v)\n",
    "        if len(self.cur) > self.window_size:\n",
    "            self.cur.pop(0)\n",
    "        if len(self.ref) < self.window_size:\n",
    "            self.ref.append(v)\n",
    "            return False\n",
    "        if len(self.cur) < self.window_size:\n",
    "            return False\n",
    "\n",
    "        ref = np.array(self.ref, float)\n",
    "        cur = np.array(self.cur, float)\n",
    "        obs = abs(ref.mean() - cur.mean())\n",
    "        both = np.concatenate([ref, cur])\n",
    "        cnt = 0\n",
    "        for _ in range(self.n_permutations):\n",
    "            np.random.shuffle(both)\n",
    "            if abs(both[:self.window_size].mean() - both[self.window_size:].mean()) >= obs:\n",
    "                cnt += 1\n",
    "        p = cnt / self.n_permutations\n",
    "        drift = p < self.alpha\n",
    "        if drift:\n",
    "            self.ref = self.cur.copy()\n",
    "        return drift\n",
    "\n",
    "# -----------------------------\n",
    "# Core routine (one dataset)\n",
    "# -----------------------------\n",
    "def run_one_dataset(data_file, data_type,\n",
    "                    base_window=800,    # initial fit window\n",
    "                    roll_window=100,    # rolling anomaly-rate window (for recovery)\n",
    "                    adapt_window=500,   # samples to refit after drift\n",
    "                    consensus_k=3,      # #detectors needed for consensus\n",
    "                    tolerance=25,       # DDA: match tolerance to consensus\n",
    "                    epsilon_sigma=2.0,  # AL: baseline + 2*std\n",
    "                    verbose=True):\n",
    "\n",
    "    # --- 1) Load & pick features\n",
    "    df = pd.read_csv(data_file)\n",
    "    if data_type == \"keystroke\":\n",
    "        feats_all = [\"dwell_time\", \"flight_time\", \"up_down_time\", \"session_duration\", \"rhythm\"]\n",
    "    else:\n",
    "        # mouse: pick what exists\n",
    "        candidates = [\"speed\", \"distance\", \"delta_x\", \"delta_y\"]\n",
    "        feats_all = [c for c in candidates if c in df.columns]\n",
    "        if not feats_all:  # fallback common minimal set\n",
    "            raise ValueError(\"No mouse features found among speed/distance/delta_x/delta_y\")\n",
    "\n",
    "    df = df.dropna(subset=feats_all)\n",
    "    X = df[feats_all].values.astype(float)\n",
    "    print(f\"{data_type.capitalize()} dataset loaded: {len(X)} rows, features={feats_all}\")\n",
    "\n",
    "    if len(X) <= base_window + roll_window + 10:\n",
    "        raise ValueError(\"Dataset too small for chosen windows. Reduce base_window/roll_window or use more data.\")\n",
    "\n",
    "    # --- 2) Scale + PCA(1) to get a compact stream signal\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "    pca = PCA(n_components=1, random_state=42)\n",
    "    pc1 = pca.fit_transform(Xs[:base_window]).ravel()\n",
    "\n",
    "    # --- 3) One-class models on the baseline\n",
    "    ocsvm = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.05)  # tighter boundary -> alerts when shifting\n",
    "    iso   = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
    "    ocsvm.fit(Xs[:base_window])\n",
    "    iso.fit(Xs[:base_window])\n",
    "\n",
    "    # establish baseline anomaly rate & std over a clean slice (just before streaming)\n",
    "    base_slice = Xs[base_window:base_window+roll_window]\n",
    "    base_pred_oc = ocsvm.predict(base_slice)          # +1 inlier, -1 outlier\n",
    "    base_anom_rate = np.mean(base_pred_oc == -1)\n",
    "    # small std estimate: binomial approx with n=roll_window\n",
    "    base_std = np.sqrt(max(base_anom_rate * (1 - base_anom_rate), 1e-6) / roll_window)\n",
    "    recovery_threshold = base_anom_rate + epsilon_sigma * base_std\n",
    "\n",
    "    # --- 4) Drift detectors\n",
    "    adwin_pc  = ADWIN(delta=0.002)\n",
    "    ph_pc     = PageHinkley(threshold=30, alpha=0.01)\n",
    "    ks_pc     = KSWIN(alpha=0.01, window_size=100, stat_size=30)\n",
    "    ddm_err   = DDM(min_num_instances=30, warning_level=2.0, drift_level=3.0)\n",
    "    pcdm_pc   = PCDM(window_size=50, n_permutations=50, alpha=0.01)\n",
    "\n",
    "    adwin_oc  = ADWIN(delta=0.002)  # on ocsvm score\n",
    "    adwin_iso = ADWIN(delta=0.002)  # on iso score\n",
    "\n",
    "    detectors = [\"ADWIN_PC\", \"PH_PC\", \"KSWIN_PC\", \"DDM_ERR\", \"PCDM_PC\", \"ADWIN_OCSVM\", \"ADWIN_ISO\"]\n",
    "    det_points = {d: [] for d in detectors}\n",
    "\n",
    "    # --- 5) Streaming + consensus + adaptation\n",
    "    consensus_points = []\n",
    "    AL_values = []\n",
    "\n",
    "    # buffers for rolling\n",
    "    roll_preds = []  # OCSVM anomaly flags in rolling window to compute recovery\n",
    "    # keep an index where we are \"waiting for recovery\"\n",
    "    waiting_recovery = False\n",
    "    recovery_start_idx = None\n",
    "\n",
    "    # Helper for anomaly scores (sign so that larger = more anomalous)\n",
    "    def ocsvm_score(x):\n",
    "        # decision_function: positive -> inlier; negative -> outlier; lower = more anomalous\n",
    "        return -ocsvm.decision_function(x.reshape(1, -1))[0]\n",
    "\n",
    "    def iso_score(x):\n",
    "        # isolationForest.decision_function: higher = more normal; lower = more anomalous\n",
    "        return -iso.decision_function(x.reshape(1, -1))[0]\n",
    "\n",
    "    # start stream from base_window\n",
    "    for i in range(base_window, len(Xs)):\n",
    "        x = Xs[i]\n",
    "        v_pc1 = float(pca.transform(x.reshape(1, -1))[0, 0])\n",
    "\n",
    "        # anomaly flags / scores\n",
    "        oc_pred = ocsvm.predict(x.reshape(1, -1))[0]      # +1/-1\n",
    "        err_flag = 1 if oc_pred == -1 else 0              # 1 => anomaly (used by DDM)\n",
    "\n",
    "        s_oc = ocsvm_score(x)\n",
    "        s_iso = iso_score(x)\n",
    "\n",
    "        # update rolling anomaly rate buffer\n",
    "        roll_preds.append(err_flag)\n",
    "        if len(roll_preds) > roll_window:\n",
    "            roll_preds.pop(0)\n",
    "        roll_rate = np.mean(roll_preds) if roll_preds else 0.0\n",
    "\n",
    "        # per-detector triggers this step\n",
    "        fired = []\n",
    "\n",
    "        # update detectors\n",
    "        if adwin_pc.update(v_pc1):   fired.append(\"ADWIN_PC\")\n",
    "        if ph_pc.update(v_pc1):      fired.append(\"PH_PC\")\n",
    "        if ks_pc.update(v_pc1):      fired.append(\"KSWIN_PC\")\n",
    "        if ddm_err.add_element(err_flag): fired.append(\"DDM_ERR\")\n",
    "        if pcdm_pc.add_element(v_pc1):    fired.append(\"PCDM_PC\")\n",
    "        if adwin_oc.update(s_oc):    fired.append(\"ADWIN_OCSVM\")\n",
    "        if adwin_iso.update(s_iso):  fired.append(\"ADWIN_ISO\")\n",
    "\n",
    "        # record detector points\n",
    "        for d in fired:\n",
    "            det_points[d].append(i)\n",
    "\n",
    "        # consensus drift?\n",
    "        if len(fired) >= consensus_k:\n",
    "            consensus_points.append(i)\n",
    "\n",
    "            # ADAPT: refit PCA + models on most recent window (use what we have)\n",
    "            start_refit = max(0, i - adapt_window)\n",
    "            Xref = Xs[start_refit:i]\n",
    "            if len(Xref) >= 50:  # minimal size\n",
    "                pca = PCA(n_components=1, random_state=42)\n",
    "                pca.fit(Xref)\n",
    "\n",
    "                ocsvm = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.05)\n",
    "                iso = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
    "                ocsvm.fit(Xref)\n",
    "                iso.fit(Xref)\n",
    "\n",
    "                # re-init streaming detectors (fresh regime)\n",
    "                adwin_pc  = ADWIN(delta=0.002)\n",
    "                ph_pc     = PageHinkley(threshold=30, alpha=0.01)\n",
    "                ks_pc     = KSWIN(alpha=0.01, window_size=100, stat_size=30)\n",
    "                ddm_err   = DDM(min_num_instances=30, warning_level=2.0, drift_level=3.0)\n",
    "                pcdm_pc   = PCDM(window_size=50, n_permutations=50, alpha=0.01)\n",
    "                adwin_oc  = ADWIN(delta=0.002)\n",
    "                adwin_iso = ADWIN(delta=0.002)\n",
    "\n",
    "                # baseline & threshold recompute from immediate post-adaptation window (if possible)\n",
    "                base_start = max(0, i - roll_window)\n",
    "                base_slice = Xs[base_start:i] if i - base_start >= roll_window else Xs[max(0, i-2*roll_window):i]\n",
    "                if len(base_slice) >= roll_window:\n",
    "                    base_pred_oc = ocsvm.predict(base_slice)\n",
    "                    base_anom_rate = np.mean(base_pred_oc == -1)\n",
    "                    base_std = np.sqrt(max(base_anom_rate * (1 - base_anom_rate), 1e-6) / roll_window)\n",
    "                    recovery_threshold = base_anom_rate + 2.0 * base_std\n",
    "\n",
    "            # start AL timer: wait until roll_rate <= threshold\n",
    "            waiting_recovery = True\n",
    "            recovery_start_idx = i\n",
    "\n",
    "        # measure AL if waiting\n",
    "        if waiting_recovery and len(roll_preds) == roll_window:\n",
    "            if roll_rate <= recovery_threshold:\n",
    "                AL_values.append(i - recovery_start_idx)\n",
    "                waiting_recovery = False\n",
    "                recovery_start_idx = None\n",
    "\n",
    "    # --- 6) Metrics: DDA per detector vs consensus points\n",
    "    # collapse consensus points (avoid duplicates within tolerance)\n",
    "    collapsed = []\n",
    "    for t in consensus_points:\n",
    "        if not collapsed or (t - collapsed[-1]) > tolerance:\n",
    "            collapsed.append(t)\n",
    "    consensus_points = collapsed\n",
    "\n",
    "    def dda_for(det_list, consensus, tol):\n",
    "        if len(det_list) == 0:\n",
    "            return 0.0\n",
    "        correct = 0\n",
    "        j = 0\n",
    "        for d in det_list:\n",
    "            # advance consensus pointer\n",
    "            while j < len(consensus) and consensus[j] < d - tol:\n",
    "                j += 1\n",
    "            if j < len(consensus) and abs(consensus[j] - d) <= tol:\n",
    "                correct += 1\n",
    "        return correct / max(1, len(det_list))\n",
    "\n",
    "    dda_scores = {d: dda_for(det_points[d], consensus_points, tolerance) for d in detectors}\n",
    "\n",
    "    # --- 7) Print results\n",
    "    print(\"\\n=== SUMMARY ===============================\")\n",
    "    print(f\"Data type: {data_type}\")\n",
    "    print(f\"Samples: {len(Xs)} | Base window: {base_window} | Roll window: {roll_window} | Adapt window: {adapt_window}\")\n",
    "    print(f\"Detectors: {detectors}\")\n",
    "    print(f\"Consensus K: {consensus_k} | Match tolerance (DDA): ±{tolerance} samples\")\n",
    "    print(\"------------------------------------------\")\n",
    "    print(f\"Total consensus drift points: {len(consensus_points)}\")\n",
    "    if consensus_points[:10]:\n",
    "        print(f\"First consensus points: {consensus_points[:10]}\")\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"Per-detector detections:\")\n",
    "    for d in detectors:\n",
    "        print(f\"  {d:12s}: {len(det_points[d])} detections  |  DDA vs consensus: {dda_scores[d]:.3f}\")\n",
    "    print(\"------------------------------------------\")\n",
    "    if AL_values:\n",
    "        print(f\"AL (Adaptation Latency) count: {len(AL_values)}\")\n",
    "        print(f\"AL mean: {np.mean(AL_values):.1f} | median: {np.median(AL_values):.1f} | min: {np.min(AL_values)} | max: {np.max(AL_values)}\")\n",
    "        print(f\"First 10 ALs: {AL_values[:10]}\")\n",
    "    else:\n",
    "        print(\"No AL measured (no consensus drifts or no recovery observed).\")\n",
    "    print(\"==========================================\\n\")\n",
    "\n",
    "    # Return (if you want to inspect programmatically)\n",
    "    return {\n",
    "        \"detector_points\": det_points,\n",
    "        \"consensus_points\": consensus_points,\n",
    "        \"dda\": dda_scores,\n",
    "        \"AL\": AL_values,\n",
    "        \"baseline_rate\": base_anom_rate,\n",
    "        \"recovery_threshold\": recovery_threshold\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Run both datasets\n",
    "# -----------------------------\n",
    "datasets = {\n",
    "    \"keystroke\": \"/Users/festusedward-n/Documents/Datasets/imputed_keystroke_data.csv\",\n",
    "    \"mouse\": \"/Users/festusedward-n/Documents/Datasets/mouse_modified_trimmed_clean_imputed.csv\",\n",
    "}\n",
    "\n",
    "# You can tweak per-dataset windows if needed (mouse is larger)\n",
    "results_all = {}\n",
    "for dtype, path in datasets.items():\n",
    "    if dtype == \"mouse\":\n",
    "        res = run_one_dataset(path, dtype,\n",
    "                              base_window=1500, roll_window=200, adapt_window=800,\n",
    "                              consensus_k=3, tolerance=30, epsilon_sigma=2.0)\n",
    "    else:\n",
    "        res = run_one_dataset(path, dtype,\n",
    "                              base_window=800, roll_window=100, adapt_window=500,\n",
    "                              consensus_k=3, tolerance=25, epsilon_sigma=2.0)\n",
    "    results_all[dtype] = res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc39ce87-b2b4-46cf-b9d9-da75ff5ebb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keystroke dataset loaded: 19996 rows\n",
      "\n",
      "Drift Points Summary for keystroke:\n",
      "  ADWIN: 0 drifts detected, first 5 at []\n",
      "  PageHinkley: 0 drifts detected, first 5 at []\n",
      "  KSWIN: 0 drifts detected, first 5 at []\n",
      "  DDM: 0 drifts detected, first 5 at []\n",
      "  EDDM: 0 drifts detected, first 5 at []\n",
      "  PCDM: 547 drifts detected, first 5 at [90, 105, 116, 356, 380]\n",
      "\n",
      "Mouse dataset loaded: 252397 rows\n",
      "\n",
      "Drift Points Summary for mouse:\n",
      "  ADWIN: 0 drifts detected, first 5 at []\n",
      "  PageHinkley: 0 drifts detected, first 5 at []\n",
      "  KSWIN: 0 drifts detected, first 5 at []\n",
      "  DDM: 0 drifts detected, first 5 at []\n",
      "  EDDM: 0 drifts detected, first 5 at []\n",
      "  PCDM: 5055 drifts detected, first 5 at [60, 71, 84, 98, 108]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from river.drift import ADWIN, PageHinkley, KSWIN\n",
    "\n",
    "# -----------------------------\n",
    "# Custom Detectors\n",
    "# -----------------------------\n",
    "class DDM:\n",
    "    def __init__(self, min_num_instances=30, warning_level=2.0, drift_level=3.0):\n",
    "        self.min_num_instances = min_num_instances\n",
    "        self.warning_level = warning_level\n",
    "        self.drift_level = drift_level\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "        self.n = 0\n",
    "        self.drift_detected = False\n",
    "        self.mean_min = float('inf')\n",
    "        self.std_min = float('inf')\n",
    "\n",
    "    def add_element(self, error):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = error\n",
    "            self.std = 0.0\n",
    "        else:\n",
    "            old_mean = self.mean\n",
    "            self.mean += (error - old_mean) / self.n\n",
    "            self.std = np.sqrt(\n",
    "                (self.std**2 * (self.n - 1) + (error - self.mean) * (error - old_mean)) / self.n\n",
    "            )\n",
    "        if self.n >= self.min_num_instances:\n",
    "            if self.mean + self.std > self.mean_min + self.drift_level * self.std_min:\n",
    "                self.drift_detected = True\n",
    "            else:\n",
    "                self.drift_detected = False\n",
    "            if not self.drift_detected:\n",
    "                self.mean_min = min(self.mean_min, self.mean)\n",
    "                self.std_min = min(self.std_min, self.std)\n",
    "        return self.drift_detected\n",
    "\n",
    "\n",
    "class EDDM:\n",
    "    def __init__(self, min_num_instances=30, warning_level=0.95, drift_level=0.9):\n",
    "        self.min_num_instances = min_num_instances\n",
    "        self.warning_level = warning_level\n",
    "        self.drift_level = drift_level\n",
    "        self.n = 0\n",
    "        self.last_error = 0\n",
    "        self.distances = []\n",
    "        self.drift_detected = False\n",
    "        self.max_mean = 0.0\n",
    "\n",
    "    def add_element(self, prediction, true_label):\n",
    "        error = 1 if prediction != true_label else 0\n",
    "        if self.n > 0 and error == 1:\n",
    "            self.distances.append(self.n - self.last_error)\n",
    "        if error == 1:\n",
    "            self.last_error = self.n\n",
    "        self.n += 1\n",
    "        if len(self.distances) > 1:\n",
    "            mean = np.mean(self.distances)\n",
    "            std = np.std(self.distances)\n",
    "            if self.n >= self.min_num_instances:\n",
    "                m = (mean + 2 * std) / self.max_mean if self.max_mean > 0 else float(\"inf\")\n",
    "                if m < self.drift_level:\n",
    "                    self.drift_detected = True\n",
    "                else:\n",
    "                    self.drift_detected = False\n",
    "                if not self.drift_detected:\n",
    "                    self.max_mean = max(self.max_mean, mean + 2 * std)\n",
    "        return self.drift_detected\n",
    "\n",
    "\n",
    "class PCDM:\n",
    "    def __init__(self, window_size=50, n_permutations=100, alpha=0.01):\n",
    "        self.window_size = window_size\n",
    "        self.n_permutations = n_permutations\n",
    "        self.alpha = alpha\n",
    "        self.reference_window = []\n",
    "        self.current_window = []\n",
    "        self.drift_detected = False\n",
    "\n",
    "    def add_element(self, value):\n",
    "        self.current_window.append(value)\n",
    "        if len(self.current_window) > self.window_size:\n",
    "            self.current_window.pop(0)\n",
    "        if len(self.reference_window) < self.window_size:\n",
    "            self.reference_window.append(value)\n",
    "            return False\n",
    "        if len(self.current_window) == self.window_size:\n",
    "            stat, p_value = self._permutation_test()\n",
    "            self.drift_detected = p_value < self.alpha\n",
    "            if self.drift_detected:\n",
    "                self.reference_window = self.current_window.copy()\n",
    "        return self.drift_detected\n",
    "\n",
    "    def _permutation_test(self):\n",
    "        ref = np.array(self.reference_window)\n",
    "        curr = np.array(self.current_window)\n",
    "        observed_diff = np.abs(np.mean(ref) - np.mean(curr))\n",
    "        combined = np.concatenate([ref, curr])\n",
    "        perm_diffs = []\n",
    "        for _ in range(self.n_permutations):\n",
    "            np.random.shuffle(combined)\n",
    "            perm_ref = combined[: self.window_size]\n",
    "            perm_curr = combined[self.window_size :]\n",
    "            perm_diffs.append(np.abs(np.mean(perm_ref) - np.mean(perm_curr)))\n",
    "        p_value = np.sum(np.array(perm_diffs) >= observed_diff) / self.n_permutations\n",
    "        return observed_diff, p_value\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Drift detection pipeline\n",
    "# -----------------------------\n",
    "def detect_natural_drift(data_file, data_type):\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(f\"\\n{data_type.capitalize()} dataset loaded: {len(df)} rows\")\n",
    "\n",
    "    # Choose features dynamically\n",
    "    if data_type == \"keystroke\":\n",
    "        features = [\"dwell_time\", \"flight_time\", \"up_down_time\", \"session_duration\", \"rhythm\"]\n",
    "    else:  # mouse\n",
    "        possible_features = [\"speed\", \"distance\", \"delta_x\", \"delta_y\"]\n",
    "        features = [f for f in possible_features if f in df.columns]\n",
    "\n",
    "    if not features:\n",
    "        raise ValueError(f\"No valid features found in {data_type} dataset!\")\n",
    "\n",
    "    X = df[features].dropna().values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Initialize detectors\n",
    "    detectors = {\n",
    "        \"ADWIN\": ADWIN(delta=0.002),\n",
    "        \"PageHinkley\": PageHinkley(threshold=30, alpha=0.01),\n",
    "        \"KSWIN\": KSWIN(alpha=0.005, window_size=100),\n",
    "        \"DDM\": DDM(),\n",
    "        \"EDDM\": EDDM(),\n",
    "        \"PCDM\": PCDM()\n",
    "    }\n",
    "\n",
    "    drift_points = {name: [] for name in detectors}\n",
    "\n",
    "    # Sequentially feed samples\n",
    "    for i, x in enumerate(X_scaled):\n",
    "        for name, det in detectors.items():\n",
    "            if name in [\"DDM\", \"EDDM\"]:\n",
    "                pred = 0 if i % 2 == 0 else 1\n",
    "                drift = det.add_element(pred, 1) if name == \"EDDM\" else det.add_element(pred)\n",
    "            elif name == \"PCDM\":\n",
    "                drift = det.add_element(np.mean(x))\n",
    "            else:\n",
    "                drift = det.update(np.mean(x))\n",
    "            if drift:\n",
    "                drift_points[name].append(i)\n",
    "\n",
    "    # Print results directly\n",
    "    print(f\"\\nDrift Points Summary for {data_type}:\")\n",
    "    for det, points in drift_points.items():\n",
    "        print(f\"  {det}: {len(points)} drifts detected, first 5 at {points[:5]}\")\n",
    "    return drift_points\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run for both datasets\n",
    "# -----------------------------\n",
    "datasets = {\n",
    "    \"keystroke\": \"/Users/festusedward-n/Documents/Datasets/imputed_keystroke_data.csv\",\n",
    "    \"mouse\": \"/Users/festusedward-n/Documents/Datasets/mouse_modified_trimmed_clean_imputed.csv\",\n",
    "}\n",
    "\n",
    "for dtype, path in datasets.items():\n",
    "    detect_natural_drift(path, dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b611aea-1e38-4a60-a5cd-63da393a6dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d17e5f-16b4-4bd7-bedd-73031f5a9a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c4512e-60e8-4f74-a5fa-b82170fdf00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b621a6a-5ef8-4f1a-b411-67b46b9c712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keystroke dataset loaded: 19996 rows\n",
      "\n",
      "=== SUMMARY ===============================\n",
      "Data type: keystroke\n",
      "Samples: 19996 | Features: ['dwell_time', 'flight_time', 'up_down_time', 'session_duration', 'rhythm']\n",
      "------------------------------------------\n",
      "  ADWIN_PC    : 0 detections\n",
      "  PH_PC       : 0 detections\n",
      "  KSWIN_PC    : 0 detections\n",
      "  DDM_ERR     : 19721 detections\n",
      "  PCDM_PC     : 902 detections\n",
      "  ADWIN_OCSVM : 0 detections\n",
      "  ADWIN_ISO   : 0 detections\n",
      "\n",
      "Adaptation Latency (mean over drifts): 222.98 samples\n",
      "==========================================\n",
      "\n",
      "Mouse dataset loaded: 252397 rows\n",
      "\n",
      "=== SUMMARY ===============================\n",
      "Data type: mouse\n",
      "Samples: 252397 | Features: ['speed', 'distance']\n",
      "------------------------------------------\n",
      "  ADWIN_PC    : 0 detections\n",
      "  PH_PC       : 0 detections\n",
      "  KSWIN_PC    : 0 detections\n",
      "  DDM_ERR     : 251311 detections\n",
      "  PCDM_PC     : 8079 detections\n",
      "  ADWIN_OCSVM : 0 detections\n",
      "  ADWIN_ISO   : 0 detections\n",
      "\n",
      "Adaptation Latency (mean over drifts): 201.00 samples\n",
      "==========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from river.drift import ADWIN, PageHinkley, KSWIN\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# -----------------------------\n",
    "# Custom detectors\n",
    "# -----------------------------\n",
    "class DDM:\n",
    "    def __init__(self, min_num_instances=30, warning_level=2.0, drift_level=3.0):\n",
    "        self.min_num_instances = min_num_instances\n",
    "        self.warning_level = warning_level\n",
    "        self.drift_level = drift_level\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "        self.n = 0\n",
    "        self.drift_detected = False\n",
    "        self.mean_min = float('inf')\n",
    "        self.std_min = float('inf')\n",
    "\n",
    "    def add_element(self, error):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = error\n",
    "            self.std = 0.0\n",
    "        else:\n",
    "            old_mean = self.mean\n",
    "            self.mean += (error - old_mean) / self.n\n",
    "            self.std = np.sqrt(\n",
    "                (self.std**2 * (self.n - 1) + (error - self.mean) * (error - old_mean)) / self.n\n",
    "            )\n",
    "        if self.n >= self.min_num_instances:\n",
    "            if self.mean + self.std > self.mean_min + self.drift_level * self.std_min:\n",
    "                self.drift_detected = True\n",
    "            else:\n",
    "                self.drift_detected = False\n",
    "            if not self.drift_detected:\n",
    "                self.mean_min = min(self.mean_min, self.mean)\n",
    "                self.std_min = min(self.std_min, self.std)\n",
    "        return self.drift_detected\n",
    "\n",
    "\n",
    "class PCDM:\n",
    "    def __init__(self, window_size=50, n_permutations=50, alpha=0.05):\n",
    "        self.window_size = window_size\n",
    "        self.n_permutations = n_permutations\n",
    "        self.alpha = alpha\n",
    "        self.reference_window = []\n",
    "        self.current_window = []\n",
    "        self.drift_detected = False\n",
    "\n",
    "    def add_element(self, value):\n",
    "        self.current_window.append(value)\n",
    "        if len(self.current_window) > self.window_size:\n",
    "            self.current_window.pop(0)\n",
    "        if len(self.reference_window) < self.window_size:\n",
    "            self.reference_window.append(value)\n",
    "            return False\n",
    "        if len(self.current_window) == self.window_size:\n",
    "            stat, p_value = self._permutation_test()\n",
    "            self.drift_detected = p_value < self.alpha\n",
    "            if self.drift_detected:\n",
    "                self.reference_window = self.current_window.copy()\n",
    "        return self.drift_detected\n",
    "\n",
    "    def _permutation_test(self):\n",
    "        ref = np.array(self.reference_window)\n",
    "        curr = np.array(self.current_window)\n",
    "        observed_diff = np.abs(np.mean(ref) - np.mean(curr))\n",
    "        combined = np.concatenate([ref, curr])\n",
    "        perm_diffs = []\n",
    "        for _ in range(self.n_permutations):\n",
    "            np.random.shuffle(combined)\n",
    "            perm_ref = combined[: self.window_size]\n",
    "            perm_curr = combined[self.window_size :]\n",
    "            perm_diffs.append(np.abs(np.mean(perm_ref) - np.mean(perm_curr)))\n",
    "        p_value = np.sum(np.array(perm_diffs) >= observed_diff) / self.n_permutations\n",
    "        return observed_diff, p_value\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Drift detection + AL pipeline\n",
    "# -----------------------------\n",
    "def detect_natural_drift(data_file, data_type):\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(f\"{data_type.capitalize()} dataset loaded: {len(df)} rows\")\n",
    "\n",
    "    # Choose features dynamically\n",
    "    if data_type == \"keystroke\":\n",
    "        features = [\"dwell_time\", \"flight_time\", \"up_down_time\", \"session_duration\", \"rhythm\"]\n",
    "    else:  # mouse\n",
    "        possible_features = [\"speed\", \"distance\", \"delta_x\", \"delta_y\"]\n",
    "        features = [f for f in possible_features if f in df.columns]\n",
    "\n",
    "    if not features:\n",
    "        raise ValueError(f\"No valid features found in {data_type} dataset!\")\n",
    "\n",
    "    X = df[features].dropna().values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Base anomaly models\n",
    "    ocsvm = OneClassSVM(nu=0.01, gamma=\"scale\")\n",
    "    iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "    ocsvm.fit(X_scaled[:1000])\n",
    "    iso.fit(X_scaled[:1000])\n",
    "\n",
    "    # Initialize detectors\n",
    "    detectors = {\n",
    "        \"ADWIN_PC\": ADWIN(delta=0.01),\n",
    "        \"PH_PC\": PageHinkley(threshold=10, alpha=0.01),\n",
    "        \"KSWIN_PC\": KSWIN(alpha=0.1, window_size=100),\n",
    "        \"DDM_ERR\": DDM(),\n",
    "        \"PCDM_PC\": PCDM(alpha=0.05),\n",
    "        \"ADWIN_OCSVM\": ADWIN(delta=0.01),\n",
    "        \"ADWIN_ISO\": ADWIN(delta=0.01),\n",
    "    }\n",
    "\n",
    "    drift_points = {name: [] for name in detectors}\n",
    "    AL_values = []\n",
    "\n",
    "    in_recovery = False\n",
    "    recovery_start = None\n",
    "\n",
    "    for i, x in enumerate(X_scaled):\n",
    "        x_val = np.mean(x)\n",
    "        err_ocsvm = 1 if ocsvm.predict([x])[0] == -1 else 0\n",
    "        err_iso = 1 if iso.predict([x])[0] == -1 else 0\n",
    "\n",
    "        drift_detected = False\n",
    "        for name, det in detectors.items():\n",
    "            if name == \"DDM_ERR\":\n",
    "                drift = det.add_element(err_ocsvm or err_iso)\n",
    "            elif name == \"PCDM_PC\":\n",
    "                drift = det.add_element(x_val)\n",
    "            elif name == \"ADWIN_OCSVM\":\n",
    "                drift = det.update(err_ocsvm)\n",
    "            elif name == \"ADWIN_ISO\":\n",
    "                drift = det.update(err_iso)\n",
    "            else:\n",
    "                drift = det.update(x_val)\n",
    "            if drift:\n",
    "                drift_detected = True\n",
    "                drift_points[name].append(i)\n",
    "\n",
    "        # Adaptation Latency: start recovery after drift\n",
    "        if drift_detected and not in_recovery:\n",
    "            in_recovery = True\n",
    "            recovery_start = i\n",
    "\n",
    "        if in_recovery:\n",
    "            # Check if models recover (low anomaly rate over last 200 samples)\n",
    "            if i - recovery_start > 200:\n",
    "                recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled[i-200:i]]\n",
    "                if np.mean(recent_errs) < 0.05:\n",
    "                    AL = i - recovery_start\n",
    "                    AL_values.append(AL)\n",
    "                    in_recovery = False\n",
    "\n",
    "    print(\"\\n=== SUMMARY ===============================\")\n",
    "    print(f\"Data type: {data_type}\")\n",
    "    print(f\"Samples: {len(X_scaled)} | Features: {features}\")\n",
    "    print(\"------------------------------------------\")\n",
    "    for det, pts in drift_points.items():\n",
    "        print(f\"  {det:12}: {len(pts)} detections\")\n",
    "    if AL_values:\n",
    "        print(f\"\\nAdaptation Latency (mean over drifts): {np.mean(AL_values):.2f} samples\")\n",
    "    else:\n",
    "        print(\"\\nNo AL measured (no recovery detected).\")\n",
    "    print(\"==========================================\\n\")\n",
    "\n",
    "    return drift_points, AL_values\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run for both datasets\n",
    "# -----------------------------\n",
    "datasets = {\n",
    "    \"keystroke\": \"/Users/festusedward-n/Documents/Datasets/imputed_keystroke_data.csv\",\n",
    "    \"mouse\": \"/Users/festusedward-n/Documents/Datasets/mouse_modified_trimmed_clean_imputed.csv\",\n",
    "}\n",
    "\n",
    "for dtype, path in datasets.items():\n",
    "    drift_points, AL = detect_natural_drift(path, dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb6cd74-64ea-4fef-9c64-26610a489dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fb92cd6-6c8d-4236-a71b-ea0f17b46885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keystroke dataset loaded: 19996 rows\n",
      "\n",
      "=== SUMMARY ===============================\n",
      "Data type: keystroke\n",
      "Samples: 19996 | Features: ['dwell_time', 'flight_time', 'up_down_time', 'session_duration', 'rhythm']\n",
      "------------------------------------------\n",
      "  ADWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  PH_PC       : 0 detections (FAR=0.00%)\n",
      "  KSWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  DDM_ERR     : 19721 detections (FAR=98.62%)\n",
      "  PCDM_PC     : 902 detections (FAR=4.51%)\n",
      "  ADWIN_OCSVM : 0 detections (FAR=0.00%)\n",
      "  ADWIN_ISO   : 0 detections (FAR=0.00%)\n",
      "\n",
      "Adaptation Latency (AL): 222.97 samples (1.12%)\n",
      "Detection Delay Accuracy (DDA): 222.97 samples\n",
      "Recovery Accuracy (RA): 99.96%\n",
      "==========================================\n",
      "\n",
      "Mouse dataset loaded: 252397 rows\n",
      "\n",
      "=== SUMMARY ===============================\n",
      "Data type: mouse\n",
      "Samples: 252397 | Features: ['speed', 'distance']\n",
      "------------------------------------------\n",
      "  ADWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  PH_PC       : 0 detections (FAR=0.00%)\n",
      "  KSWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  DDM_ERR     : 251311 detections (FAR=99.57%)\n",
      "  PCDM_PC     : 8004 detections (FAR=3.17%)\n",
      "  ADWIN_OCSVM : 0 detections (FAR=0.00%)\n",
      "  ADWIN_ISO   : 0 detections (FAR=0.00%)\n",
      "\n",
      "Adaptation Latency (AL): 201.00 samples (0.08%)\n",
      "Detection Delay Accuracy (DDA): 201.00 samples\n",
      "Recovery Accuracy (RA): 100.00%\n",
      "==========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from river.drift import ADWIN, PageHinkley, KSWIN\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# -----------------------------\n",
    "# Custom detectors\n",
    "# -----------------------------\n",
    "class DDM:\n",
    "    def __init__(self, min_num_instances=30, warning_level=2.0, drift_level=3.0):\n",
    "        self.min_num_instances = min_num_instances\n",
    "        self.warning_level = warning_level\n",
    "        self.drift_level = drift_level\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "        self.n = 0\n",
    "        self.drift_detected = False\n",
    "        self.mean_min = float('inf')\n",
    "        self.std_min = float('inf')\n",
    "\n",
    "    def add_element(self, error):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = error\n",
    "            self.std = 0.0\n",
    "        else:\n",
    "            old_mean = self.mean\n",
    "            self.mean += (error - old_mean) / self.n\n",
    "            self.std = np.sqrt(\n",
    "                (self.std**2 * (self.n - 1) + (error - self.mean) * (error - old_mean)) / self.n\n",
    "            )\n",
    "        if self.n >= self.min_num_instances:\n",
    "            if self.mean + self.std > self.mean_min + self.drift_level * self.std_min:\n",
    "                self.drift_detected = True\n",
    "            else:\n",
    "                self.drift_detected = False\n",
    "            if not self.drift_detected:\n",
    "                self.mean_min = min(self.mean_min, self.mean)\n",
    "                self.std_min = min(self.std_min, self.std)\n",
    "        return self.drift_detected\n",
    "\n",
    "\n",
    "class PCDM:\n",
    "    def __init__(self, window_size=50, n_permutations=50, alpha=0.05):\n",
    "        self.window_size = window_size\n",
    "        self.n_permutations = n_permutations\n",
    "        self.alpha = alpha\n",
    "        self.reference_window = []\n",
    "        self.current_window = []\n",
    "        self.drift_detected = False\n",
    "\n",
    "    def add_element(self, value):\n",
    "        self.current_window.append(value)\n",
    "        if len(self.current_window) > self.window_size:\n",
    "            self.current_window.pop(0)\n",
    "        if len(self.reference_window) < self.window_size:\n",
    "            self.reference_window.append(value)\n",
    "            return False\n",
    "        if len(self.current_window) == self.window_size:\n",
    "            stat, p_value = self._permutation_test()\n",
    "            self.drift_detected = p_value < self.alpha\n",
    "            if self.drift_detected:\n",
    "                self.reference_window = self.current_window.copy()\n",
    "        return self.drift_detected\n",
    "\n",
    "    def _permutation_test(self):\n",
    "        ref = np.array(self.reference_window)\n",
    "        curr = np.array(self.current_window)\n",
    "        observed_diff = np.abs(np.mean(ref) - np.mean(curr))\n",
    "        combined = np.concatenate([ref, curr])\n",
    "        perm_diffs = []\n",
    "        for _ in range(self.n_permutations):\n",
    "            np.random.shuffle(combined)\n",
    "            perm_ref = combined[: self.window_size]\n",
    "            perm_curr = combined[self.window_size :]\n",
    "            perm_diffs.append(np.abs(np.mean(perm_ref) - np.mean(perm_curr)))\n",
    "        p_value = np.sum(np.array(perm_diffs) >= observed_diff) / self.n_permutations\n",
    "        return observed_diff, p_value\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Drift detection + metrics pipeline\n",
    "# -----------------------------\n",
    "def detect_natural_drift(data_file, data_type):\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(f\"{data_type.capitalize()} dataset loaded: {len(df)} rows\")\n",
    "\n",
    "    # Choose features dynamically\n",
    "    if data_type == \"keystroke\":\n",
    "        features = [\"dwell_time\", \"flight_time\", \"up_down_time\", \"session_duration\", \"rhythm\"]\n",
    "    else:  # mouse\n",
    "        possible_features = [\"speed\", \"distance\", \"delta_x\", \"delta_y\"]\n",
    "        features = [f for f in possible_features if f in df.columns]\n",
    "\n",
    "    if not features:\n",
    "        raise ValueError(f\"No valid features found in {data_type} dataset!\")\n",
    "\n",
    "    X = df[features].dropna().values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Base anomaly models\n",
    "    ocsvm = OneClassSVM(nu=0.01, gamma=\"scale\")\n",
    "    iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "    ocsvm.fit(X_scaled[:1000])\n",
    "    iso.fit(X_scaled[:1000])\n",
    "\n",
    "    # Initialize detectors\n",
    "    detectors = {\n",
    "        \"ADWIN_PC\": ADWIN(delta=0.01),\n",
    "        \"PH_PC\": PageHinkley(threshold=10, alpha=0.01),\n",
    "        \"KSWIN_PC\": KSWIN(alpha=0.1, window_size=100),\n",
    "        \"DDM_ERR\": DDM(),\n",
    "        \"PCDM_PC\": PCDM(alpha=0.05),\n",
    "        \"ADWIN_OCSVM\": ADWIN(delta=0.01),\n",
    "        \"ADWIN_ISO\": ADWIN(delta=0.01),\n",
    "    }\n",
    "\n",
    "    drift_points = {name: [] for name in detectors}\n",
    "    AL_values, DDA_values, RA_values = [], [], []\n",
    "    FAR_values = {}\n",
    "\n",
    "    in_recovery = False\n",
    "    recovery_start = None\n",
    "    baseline_acc = None\n",
    "\n",
    "    for i, x in enumerate(X_scaled):\n",
    "        x_val = np.mean(x)\n",
    "        err_ocsvm = 1 if ocsvm.predict([x])[0] == -1 else 0\n",
    "        err_iso = 1 if iso.predict([x])[0] == -1 else 0\n",
    "\n",
    "        drift_detected = False\n",
    "        for name, det in detectors.items():\n",
    "            if name == \"DDM_ERR\":\n",
    "                drift = det.add_element(err_ocsvm or err_iso)\n",
    "            elif name == \"PCDM_PC\":\n",
    "                drift = det.add_element(x_val)\n",
    "            elif name == \"ADWIN_OCSVM\":\n",
    "                drift = det.update(err_ocsvm)\n",
    "            elif name == \"ADWIN_ISO\":\n",
    "                drift = det.update(err_iso)\n",
    "            else:\n",
    "                drift = det.update(x_val)\n",
    "            if drift:\n",
    "                drift_detected = True\n",
    "                drift_points[name].append(i)\n",
    "\n",
    "        # Adaptation metrics\n",
    "        if drift_detected and not in_recovery:\n",
    "            in_recovery = True\n",
    "            recovery_start = i\n",
    "            # Baseline before drift\n",
    "            recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled[max(0, i-200):i]]\n",
    "            baseline_acc = 1 - np.mean(recent_errs) if len(recent_errs) > 0 else None\n",
    "\n",
    "        if in_recovery:\n",
    "            if i - recovery_start > 200:\n",
    "                recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled[i-200:i]]\n",
    "                if np.mean(recent_errs) < 0.05:\n",
    "                    AL = i - recovery_start\n",
    "                    AL_values.append(AL)\n",
    "\n",
    "                    # DDA = approximated as AL\n",
    "                    DDA_values.append(AL)\n",
    "\n",
    "                    # RA = accuracy after recovery vs baseline\n",
    "                    if baseline_acc is not None:\n",
    "                        rec_acc = 1 - np.mean(recent_errs)\n",
    "                        RA_values.append(rec_acc / baseline_acc * 100)\n",
    "\n",
    "                    in_recovery = False\n",
    "\n",
    "    # FAR = detections / total samples\n",
    "    total_samples = len(X_scaled)\n",
    "    for det, pts in drift_points.items():\n",
    "        FAR_values[det] = len(pts) / total_samples * 100\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n=== SUMMARY ===============================\")\n",
    "    print(f\"Data type: {data_type}\")\n",
    "    print(f\"Samples: {len(X_scaled)} | Features: {features}\")\n",
    "    print(\"------------------------------------------\")\n",
    "    for det, pts in drift_points.items():\n",
    "        print(f\"  {det:12}: {len(pts)} detections (FAR={FAR_values[det]:.2f}%)\")\n",
    "\n",
    "    if AL_values:\n",
    "        print(f\"\\nAdaptation Latency (AL): {np.mean(AL_values):.2f} samples \"\n",
    "              f\"({np.mean(AL_values)/len(X_scaled)*100:.2f}%)\")\n",
    "        print(f\"Detection Delay Accuracy (DDA): {np.mean(DDA_values):.2f} samples\")\n",
    "        print(f\"Recovery Accuracy (RA): {np.mean(RA_values):.2f}%\")\n",
    "    else:\n",
    "        print(\"\\nNo AL/DDA/RA measured (no recovery detected).\")\n",
    "    print(\"==========================================\\n\")\n",
    "\n",
    "    return drift_points, AL_values, DDA_values, RA_values, FAR_values\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run for both datasets\n",
    "# -----------------------------\n",
    "datasets = {\n",
    "    \"keystroke\": \"/Users/festusedward-n/Documents/Datasets/imputed_keystroke_data.csv\",\n",
    "    \"mouse\": \"/Users/festusedward-n/Documents/Datasets/mouse_modified_trimmed_clean_imputed.csv\",\n",
    "}\n",
    "\n",
    "for dtype, path in datasets.items():\n",
    "    drift_points, AL, DDA, RA, FAR = detect_natural_drift(path, dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7572961c-5546-43c2-9e9c-57c2ebc343ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b64c8-c21f-4fd1-92d7-7a0d382537c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcb6b6c5-7c81-4a96-b65d-d41220c0b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keystroke dataset loaded: 19996 rows\n",
      "\n",
      "=== SUMMARY ===============================\n",
      "Data type: keystroke\n",
      "Samples: 19996 | Features: ['dwell_time', 'flight_time', 'up_down_time', 'session_duration', 'rhythm']\n",
      "------------------------------------------\n",
      "  ADWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  PH_PC       : 0 detections (FAR=0.00%)\n",
      "  KSWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  DDM_ERR     : 19069 detections (FAR=95.36%)\n",
      "  PCDM_PC     : 889 detections (FAR=4.45%)\n",
      "  ADWIN_OCSVM : 0 detections (FAR=0.00%)\n",
      "  ADWIN_ISO   : 0 detections (FAR=0.00%)\n",
      "\n",
      "Adaptation Latency (AL): 224.70 samples\n",
      "Detection Delay Accuracy (DDA): 224.70 samples\n",
      "Recovery Accuracy (RA): 2.20%\n",
      "Average Error Rate (AER): 9.01%\n",
      "Drift Density (DD): 99.81%\n",
      "==========================================\n",
      "\n",
      "Mouse dataset loaded: 252397 rows\n",
      "\n",
      "=== SUMMARY ===============================\n",
      "Data type: mouse\n",
      "Samples: 252397 | Features: ['speed', 'distance']\n",
      "------------------------------------------\n",
      "  ADWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  PH_PC       : 0 detections (FAR=0.00%)\n",
      "  KSWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  DDM_ERR     : 251310 detections (FAR=99.57%)\n",
      "  PCDM_PC     : 8054 detections (FAR=3.19%)\n",
      "  ADWIN_OCSVM : 0 detections (FAR=0.00%)\n",
      "  ADWIN_ISO   : 0 detections (FAR=0.00%)\n",
      "\n",
      "Adaptation Latency (AL): 485.82 samples\n",
      "Detection Delay Accuracy (DDA): 485.82 samples\n",
      "Recovery Accuracy (RA): 1.48%\n",
      "Average Error Rate (AER): 26.40%\n",
      "Drift Density (DD): 102.76%\n",
      "==========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from river.drift import ADWIN, PageHinkley, KSWIN\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# -----------------------------\n",
    "# Custom detectors\n",
    "# -----------------------------\n",
    "class DDM:\n",
    "    def __init__(self, min_num_instances=30, warning_level=2.0, drift_level=3.0):\n",
    "        self.min_num_instances = min_num_instances\n",
    "        self.warning_level = warning_level\n",
    "        self.drift_level = drift_level\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "        self.n = 0\n",
    "        self.drift_detected = False\n",
    "        self.mean_min = float('inf')\n",
    "        self.std_min = float('inf')\n",
    "\n",
    "    def add_element(self, error):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = error\n",
    "            self.std = 0.0\n",
    "        else:\n",
    "            old_mean = self.mean\n",
    "            self.mean += (error - old_mean) / self.n\n",
    "            self.std = np.sqrt(\n",
    "                (self.std**2 * (self.n - 1) + (error - self.mean) * (error - old_mean)) / self.n\n",
    "            )\n",
    "        if self.n >= self.min_num_instances:\n",
    "            if self.mean + self.std > self.mean_min + self.drift_level * self.std_min:\n",
    "                self.drift_detected = True\n",
    "            else:\n",
    "                self.drift_detected = False\n",
    "            if not self.drift_detected:\n",
    "                self.mean_min = min(self.mean_min, self.mean)\n",
    "                self.std_min = min(self.std_min, self.std)\n",
    "        return self.drift_detected\n",
    "\n",
    "\n",
    "class PCDM:\n",
    "    def __init__(self, window_size=50, n_permutations=50, alpha=0.05):\n",
    "        self.window_size = window_size\n",
    "        self.n_permutations = n_permutations\n",
    "        self.alpha = alpha\n",
    "        self.reference_window = []\n",
    "        self.current_window = []\n",
    "        self.drift_detected = False\n",
    "\n",
    "    def add_element(self, value):\n",
    "        self.current_window.append(value)\n",
    "        if len(self.current_window) > self.window_size:\n",
    "            self.current_window.pop(0)\n",
    "        if len(self.reference_window) < self.window_size:\n",
    "            self.reference_window.append(value)\n",
    "            return False\n",
    "        if len(self.current_window) == self.window_size:\n",
    "            stat, p_value = self._permutation_test()\n",
    "            self.drift_detected = p_value < self.alpha\n",
    "            if self.drift_detected:\n",
    "                self.reference_window = self.current_window.copy()\n",
    "        return self.drift_detected\n",
    "\n",
    "    def _permutation_test(self):\n",
    "        ref = np.array(self.reference_window)\n",
    "        curr = np.array(self.current_window)\n",
    "        observed_diff = np.abs(np.mean(ref) - np.mean(curr))\n",
    "        combined = np.concatenate([ref, curr])\n",
    "        perm_diffs = []\n",
    "        for _ in range(self.n_permutations):\n",
    "            np.random.shuffle(combined)\n",
    "            perm_ref = combined[: self.window_size]\n",
    "            perm_curr = combined[self.window_size :]\n",
    "            perm_diffs.append(np.abs(np.mean(perm_ref) - np.mean(perm_curr)))\n",
    "        p_value = np.sum(np.array(perm_diffs) >= observed_diff) / self.n_permutations\n",
    "        return observed_diff, p_value\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Drift detection + metrics pipeline\n",
    "# -----------------------------\n",
    "def detect_natural_drift(data_file, data_type):\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(f\"{data_type.capitalize()} dataset loaded: {len(df)} rows\")\n",
    "\n",
    "    # Choose features dynamically\n",
    "    if data_type == \"keystroke\":\n",
    "        features = [\"dwell_time\", \"flight_time\", \"up_down_time\", \"session_duration\", \"rhythm\"]\n",
    "    else:  # mouse\n",
    "        possible_features = [\"speed\", \"distance\", \"delta_x\", \"delta_y\"]\n",
    "        features = [f for f in possible_features if f in df.columns]\n",
    "\n",
    "    if not features:\n",
    "        raise ValueError(f\"No valid features found in {data_type} dataset!\")\n",
    "\n",
    "    X = df[features].dropna().values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Base anomaly models (looser params for realism)\n",
    "    ocsvm = OneClassSVM(nu=0.05, gamma=0.1)\n",
    "    iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "    ocsvm.fit(X_scaled[:min(1000, len(X_scaled))])\n",
    "    iso.fit(X_scaled[:min(1000, len(X_scaled))])\n",
    "\n",
    "    # Initialize detectors\n",
    "    detectors = {\n",
    "        \"ADWIN_PC\": ADWIN(delta=0.01),\n",
    "        \"PH_PC\": PageHinkley(threshold=10, alpha=0.01),\n",
    "        \"KSWIN_PC\": KSWIN(alpha=0.1, window_size=100),\n",
    "        \"DDM_ERR\": DDM(),\n",
    "        \"PCDM_PC\": PCDM(alpha=0.05),\n",
    "        \"ADWIN_OCSVM\": ADWIN(delta=0.01),\n",
    "        \"ADWIN_ISO\": ADWIN(delta=0.01),\n",
    "    }\n",
    "\n",
    "    drift_points = {name: [] for name in detectors}\n",
    "    AL_values, DDA_values, RA_values = [], [], []\n",
    "    FAR_values = {}\n",
    "\n",
    "    in_recovery = False\n",
    "    recovery_start = None\n",
    "    baseline_acc = None\n",
    "\n",
    "    # Track error rates\n",
    "    all_errors = []\n",
    "\n",
    "    for i, x in enumerate(X_scaled):\n",
    "        x_val = np.mean(x)\n",
    "        err_ocsvm = 1 if ocsvm.predict([x])[0] == -1 else 0\n",
    "        err_iso = 1 if iso.predict([x])[0] == -1 else 0\n",
    "        error = max(err_ocsvm, err_iso)\n",
    "        all_errors.append(error)\n",
    "\n",
    "        drift_detected = False\n",
    "        for name, det in detectors.items():\n",
    "            if name == \"DDM_ERR\":\n",
    "                drift = det.add_element(error)\n",
    "            elif name == \"PCDM_PC\":\n",
    "                drift = det.add_element(x_val)\n",
    "            elif name == \"ADWIN_OCSVM\":\n",
    "                drift = det.update(err_ocsvm)\n",
    "            elif name == \"ADWIN_ISO\":\n",
    "                drift = det.update(err_iso)\n",
    "            else:\n",
    "                drift = det.update(x_val)\n",
    "            if drift:\n",
    "                drift_detected = True\n",
    "                drift_points[name].append(i)\n",
    "\n",
    "        # Adaptation metrics\n",
    "        if drift_detected and not in_recovery:\n",
    "            in_recovery = True\n",
    "            recovery_start = i\n",
    "            # Baseline before drift\n",
    "            recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled[max(0, i-200):i]]\n",
    "            baseline_acc = 1 - np.mean(recent_errs) if len(recent_errs) > 0 else None\n",
    "\n",
    "        if in_recovery:\n",
    "            if i - recovery_start > 200:\n",
    "                recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled[i-200:i]]\n",
    "                if np.mean(recent_errs) < 0.15:  # relaxed threshold\n",
    "                    AL = i - recovery_start\n",
    "                    AL_values.append(AL)\n",
    "                    DDA_values.append(AL)\n",
    "\n",
    "                    if baseline_acc is not None:\n",
    "                        rec_acc = 1 - np.mean(recent_errs)\n",
    "                        improvement = max(0, rec_acc - baseline_acc)\n",
    "                        RA_values.append(improvement * 100)\n",
    "\n",
    "                    in_recovery = False\n",
    "\n",
    "    # FAR = detections / total samples\n",
    "    total_samples = len(X_scaled)\n",
    "    for det, pts in drift_points.items():\n",
    "        FAR_values[det] = len(pts) / total_samples * 100\n",
    "\n",
    "    # Extra metrics\n",
    "    AER = np.mean(all_errors) * 100  # average error rate\n",
    "    drift_density = sum(len(v) for v in drift_points.values()) / total_samples * 100\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n=== SUMMARY ===============================\")\n",
    "    print(f\"Data type: {data_type}\")\n",
    "    print(f\"Samples: {len(X_scaled)} | Features: {features}\")\n",
    "    print(\"------------------------------------------\")\n",
    "    for det, pts in drift_points.items():\n",
    "        print(f\"  {det:12}: {len(pts)} detections (FAR={FAR_values[det]:.2f}%)\")\n",
    "\n",
    "    if AL_values:\n",
    "        print(f\"\\nAdaptation Latency (AL): {np.mean(AL_values):.2f} samples\")\n",
    "        print(f\"Detection Delay Accuracy (DDA): {np.mean(DDA_values):.2f} samples\")\n",
    "        print(f\"Recovery Accuracy (RA): {np.mean(RA_values):.2f}%\")\n",
    "    else:\n",
    "        print(\"\\nNo AL/DDA/RA measured (no recovery detected).\")\n",
    "\n",
    "    print(f\"Average Error Rate (AER): {AER:.2f}%\")\n",
    "    print(f\"Drift Density (DD): {drift_density:.2f}%\")\n",
    "    print(\"==========================================\\n\")\n",
    "\n",
    "    return drift_points, AL_values, DDA_values, RA_values, FAR_values, AER, drift_density\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run for both datasets\n",
    "# -----------------------------\n",
    "datasets = {\n",
    "    \"keystroke\": \"/Users/festusedward-n/Documents/Datasets/imputed_keystroke_data.csv\",\n",
    "    \"mouse\": \"/Users/festusedward-n/Documents/Datasets/mouse_modified_trimmed_clean_imputed.csv\",\n",
    "}\n",
    "\n",
    "for dtype, path in datasets.items():\n",
    "    detect_natural_drift(path, dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b500cb5-4fa3-4c7e-9c3f-9a1a9edef66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env_310)",
   "language": "python",
   "name": "pytorch_env_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
