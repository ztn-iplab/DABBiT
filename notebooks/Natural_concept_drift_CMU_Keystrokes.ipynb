{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e352de55-fb13-4539-bf91-897d4c58b5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keystroke dataset loaded: 20400 rows\n",
      "Available columns: ['subject', 'sessionIndex', 'rep', 'H.period', 'DD.period.t', 'UD.period.t', 'H.t', 'DD.t.i', 'UD.t.i', 'H.i', 'DD.i.e', 'UD.i.e', 'H.e', 'DD.e.five', 'UD.e.five', 'H.five', 'DD.five.Shift.r', 'UD.five.Shift.r', 'H.Shift.r', 'DD.Shift.r.o'] ...\n",
      "\n",
      "=== SUMMARY ===============================\n",
      "Data type: keystroke\n",
      "Samples: 20400 | Features used: ['H.period', 'DD.period.t', 'UD.period.t', 'H.t', 'DD.t.i', 'UD.t.i', 'H.i', 'DD.i.e', 'UD.i.e', 'H.e'] ...\n",
      "------------------------------------------\n",
      "  ADWIN_PC    : 0 detections\n",
      "  PH_PC       : 0 detections\n",
      "  KSWIN_PC    : 0 detections\n",
      "  DDM_ERR     : 17663 detections\n",
      "  PCDM_PC     : 599 detections\n",
      "  ADWIN_OCSVM : 0 detections\n",
      "  ADWIN_ISO   : 0 detections\n",
      "\n",
      "Adaptation Latency (mean over drifts): 2740.17 samples\n",
      "==========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from river.drift import ADWIN, PageHinkley, KSWIN\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# -----------------------------\n",
    "# Custom detectors\n",
    "# -----------------------------\n",
    "class DDM:\n",
    "    def __init__(self, min_num_instances=30, warning_level=2.0, drift_level=3.0):\n",
    "        self.min_num_instances = min_num_instances\n",
    "        self.warning_level = warning_level\n",
    "        self.drift_level = drift_level\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "        self.n = 0\n",
    "        self.drift_detected = False\n",
    "        self.mean_min = float('inf')\n",
    "        self.std_min = float('inf')\n",
    "\n",
    "    def add_element(self, error):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = error\n",
    "            self.std = 0.0\n",
    "        else:\n",
    "            old_mean = self.mean\n",
    "            self.mean += (error - old_mean) / self.n\n",
    "            self.std = np.sqrt(\n",
    "                (self.std**2 * (self.n - 1) + (error - self.mean) * (error - old_mean)) / self.n\n",
    "            )\n",
    "        if self.n >= self.min_num_instances:\n",
    "            if self.mean + self.std > self.mean_min + self.drift_level * self.std_min:\n",
    "                self.drift_detected = True\n",
    "            else:\n",
    "                self.drift_detected = False\n",
    "            if not self.drift_detected:\n",
    "                self.mean_min = min(self.mean_min, self.mean)\n",
    "                self.std_min = min(self.std_min, self.std)\n",
    "        return self.drift_detected\n",
    "\n",
    "\n",
    "class PCDM:\n",
    "    def __init__(self, window_size=50, n_permutations=50, alpha=0.05):\n",
    "        self.window_size = window_size\n",
    "        self.n_permutations = n_permutations\n",
    "        self.alpha = alpha\n",
    "        self.reference_window = []\n",
    "        self.current_window = []\n",
    "        self.drift_detected = False\n",
    "\n",
    "    def add_element(self, value):\n",
    "        self.current_window.append(value)\n",
    "        if len(self.current_window) > self.window_size:\n",
    "            self.current_window.pop(0)\n",
    "        if len(self.reference_window) < self.window_size:\n",
    "            self.reference_window.append(value)\n",
    "            return False\n",
    "        if len(self.current_window) == self.window_size:\n",
    "            stat, p_value = self._permutation_test()\n",
    "            self.drift_detected = p_value < self.alpha\n",
    "            if self.drift_detected:\n",
    "                self.reference_window = self.current_window.copy()\n",
    "        return self.drift_detected\n",
    "\n",
    "    def _permutation_test(self):\n",
    "        ref = np.array(self.reference_window)\n",
    "        curr = np.array(self.current_window)\n",
    "        observed_diff = np.abs(np.mean(ref) - np.mean(curr))\n",
    "        combined = np.concatenate([ref, curr])\n",
    "        perm_diffs = []\n",
    "        for _ in range(self.n_permutations):\n",
    "            np.random.shuffle(combined)\n",
    "            perm_ref = combined[: self.window_size]\n",
    "            perm_curr = combined[self.window_size :]\n",
    "            perm_diffs.append(np.abs(np.mean(perm_ref) - np.mean(perm_curr)))\n",
    "        p_value = np.sum(np.array(perm_diffs) >= observed_diff) / self.n_permutations\n",
    "        return observed_diff, p_value\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Drift detection + AL pipeline\n",
    "# -----------------------------\n",
    "def detect_natural_drift(data_file, data_type):\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(f\"{data_type.capitalize()} dataset loaded: {len(df)} rows\")\n",
    "    print(f\"Available columns: {list(df.columns)[:20]} ...\")  # show first 20 column names\n",
    "\n",
    "    # Dynamically select numeric features (exclude IDs)\n",
    "    exclude_cols = [\"subject\", \"sessionIndex\", \"rep\"]\n",
    "    features = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "    if not features:\n",
    "        raise ValueError(f\"No valid features found in {data_type} dataset!\")\n",
    "\n",
    "    X = df[features].dropna().values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Base anomaly models\n",
    "    ocsvm = OneClassSVM(nu=0.01, gamma=\"scale\")\n",
    "    iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "    ocsvm.fit(X_scaled[:1000])\n",
    "    iso.fit(X_scaled[:1000])\n",
    "\n",
    "    # Initialize detectors\n",
    "    detectors = {\n",
    "        \"ADWIN_PC\": ADWIN(delta=0.01),\n",
    "        \"PH_PC\": PageHinkley(threshold=10, alpha=0.01),\n",
    "        \"KSWIN_PC\": KSWIN(alpha=0.1, window_size=100),\n",
    "        \"DDM_ERR\": DDM(),\n",
    "        \"PCDM_PC\": PCDM(alpha=0.05),\n",
    "        \"ADWIN_OCSVM\": ADWIN(delta=0.01),\n",
    "        \"ADWIN_ISO\": ADWIN(delta=0.01),\n",
    "    }\n",
    "\n",
    "    drift_points = {name: [] for name in detectors}\n",
    "    AL_values = []\n",
    "\n",
    "    in_recovery = False\n",
    "    recovery_start = None\n",
    "\n",
    "    for i, x in enumerate(X_scaled):\n",
    "        x_val = np.mean(x)\n",
    "        err_ocsvm = 1 if ocsvm.predict([x])[0] == -1 else 0\n",
    "        err_iso = 1 if iso.predict([x])[0] == -1 else 0\n",
    "\n",
    "        drift_detected = False\n",
    "        for name, det in detectors.items():\n",
    "            if name == \"DDM_ERR\":\n",
    "                drift = det.add_element(err_ocsvm or err_iso)\n",
    "            elif name == \"PCDM_PC\":\n",
    "                drift = det.add_element(x_val)\n",
    "            elif name == \"ADWIN_OCSVM\":\n",
    "                drift = det.update(err_ocsvm)\n",
    "            elif name == \"ADWIN_ISO\":\n",
    "                drift = det.update(err_iso)\n",
    "            else:\n",
    "                drift = det.update(x_val)\n",
    "            if drift:\n",
    "                drift_detected = True\n",
    "                drift_points[name].append(i)\n",
    "\n",
    "        # Adaptation Latency\n",
    "        if drift_detected and not in_recovery:\n",
    "            in_recovery = True\n",
    "            recovery_start = i\n",
    "\n",
    "        if in_recovery:\n",
    "            if i - recovery_start > 200:\n",
    "                recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled[i-200:i]]\n",
    "                if np.mean(recent_errs) < 0.05:\n",
    "                    AL = i - recovery_start\n",
    "                    AL_values.append(AL)\n",
    "                    in_recovery = False\n",
    "\n",
    "    print(\"\\n=== SUMMARY ===============================\")\n",
    "    print(f\"Data type: {data_type}\")\n",
    "    print(f\"Samples: {len(X_scaled)} | Features used: {features[:10]} ...\")\n",
    "    print(\"------------------------------------------\")\n",
    "    for det, pts in drift_points.items():\n",
    "        print(f\"  {det:12}: {len(pts)} detections\")\n",
    "    if AL_values:\n",
    "        print(f\"\\nAdaptation Latency (mean over drifts): {np.mean(AL_values):.2f} samples\")\n",
    "    else:\n",
    "        print(\"\\nNo AL measured (no recovery detected).\")\n",
    "    print(\"==========================================\\n\")\n",
    "\n",
    "    return drift_points, AL_values\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run for CMU keystroke dataset\n",
    "# -----------------------------\n",
    "datasets = {\n",
    "    \"keystroke\": \"/Users/festusedward-n/Documents/Datasets/DSL-StrongPasswordData 2.csv\"\n",
    "}\n",
    "\n",
    "for dtype, path in datasets.items():\n",
    "    drift_points, AL = detect_natural_drift(path, dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596f1372-a8d0-4766-8136-5225793b6c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1972c8e-06d9-45e8-9472-bf5d55d77df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ddb61e-3185-4e5b-a395-5dcc9293dea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keystroke dataset loaded: 20400 rows\n",
      "Available columns: ['subject', 'sessionIndex', 'rep', 'H.period', 'DD.period.t', 'UD.period.t', 'H.t', 'DD.t.i', 'UD.t.i', 'H.i', 'DD.i.e', 'UD.i.e', 'H.e', 'DD.e.five', 'UD.e.five', 'H.five', 'DD.five.Shift.r', 'UD.five.Shift.r', 'H.Shift.r', 'DD.Shift.r.o'] ...\n",
      "\n",
      "=== SUMMARY ===============================\n",
      "Data type: keystroke\n",
      "Samples: 20400 | Features: 31\n",
      "------------------------------------------\n",
      "  PCDM_H.period  : 583 detections\n",
      "  PCDM_DD.period.t: 359 detections\n",
      "  PCDM_UD.period.t: 344 detections\n",
      "  PCDM_H.t       : 566 detections\n",
      "  PCDM_DD.t.i    : 338 detections\n",
      "  PCDM_UD.t.i    : 323 detections\n",
      "  PCDM_H.i       : 560 detections\n",
      "  PCDM_DD.i.e    : 321 detections\n",
      "  PCDM_UD.i.e    : 296 detections\n",
      "  PCDM_H.e       : 549 detections\n",
      "  PCDM_DD.e.five : 407 detections\n",
      "  PCDM_UD.e.five : 412 detections\n",
      "  PCDM_H.five    : 488 detections\n",
      "  PCDM_DD.five.Shift.r: 381 detections\n",
      "  PCDM_UD.five.Shift.r: 362 detections\n",
      "  PCDM_H.Shift.r : 589 detections\n",
      "  PCDM_DD.Shift.r.o: 416 detections\n",
      "  PCDM_UD.Shift.r.o: 375 detections\n",
      "  PCDM_H.o       : 558 detections\n",
      "  PCDM_DD.o.a    : 325 detections\n",
      "  PCDM_UD.o.a    : 365 detections\n",
      "  PCDM_H.a       : 590 detections\n",
      "  PCDM_DD.a.n    : 389 detections\n",
      "  PCDM_UD.a.n    : 402 detections\n",
      "  PCDM_H.n       : 561 detections\n",
      "  PCDM_DD.n.l    : 392 detections\n",
      "  PCDM_UD.n.l    : 383 detections\n",
      "  PCDM_H.l       : 564 detections\n",
      "  PCDM_DD.l.Return: 364 detections\n",
      "  PCDM_UD.l.Return: 350 detections\n",
      "  PCDM_H.Return  : 524 detections\n",
      "  DDM_ERR        : 632336 detections\n",
      "\n",
      "Adaptation Latency (mean over drifts): 2372.57 samples\n",
      "==========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from river.drift import ADWIN, PageHinkley, KSWIN\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# -----------------------------\n",
    "# Custom detectors\n",
    "# -----------------------------\n",
    "class DDM:\n",
    "    def __init__(self, min_num_instances=30, warning_level=2.0, drift_level=3.0):\n",
    "        self.min_num_instances = min_num_instances\n",
    "        self.warning_level = warning_level\n",
    "        self.drift_level = drift_level\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "        self.n = 0\n",
    "        self.drift_detected = False\n",
    "        self.mean_min = float('inf')\n",
    "        self.std_min = float('inf')\n",
    "\n",
    "    def add_element(self, error):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = error\n",
    "            self.std = 0.0\n",
    "        else:\n",
    "            old_mean = self.mean\n",
    "            self.mean += (error - old_mean) / self.n\n",
    "            self.std = np.sqrt(\n",
    "                (self.std**2 * (self.n - 1) + (error - self.mean) * (error - old_mean)) / self.n\n",
    "            )\n",
    "        if self.n >= self.min_num_instances:\n",
    "            if self.mean + self.std > self.mean_min + self.drift_level * self.std_min:\n",
    "                self.drift_detected = True\n",
    "            else:\n",
    "                self.drift_detected = False\n",
    "            if not self.drift_detected:\n",
    "                self.mean_min = min(self.mean_min, self.mean)\n",
    "                self.std_min = min(self.std_min, self.std)\n",
    "        return self.drift_detected\n",
    "\n",
    "\n",
    "class PCDM:\n",
    "    def __init__(self, window_size=50, n_permutations=50, alpha=0.05):\n",
    "        self.window_size = window_size\n",
    "        self.n_permutations = n_permutations\n",
    "        self.alpha = alpha\n",
    "        self.reference_window = []\n",
    "        self.current_window = []\n",
    "        self.drift_detected = False\n",
    "\n",
    "    def add_element(self, value):\n",
    "        self.current_window.append(value)\n",
    "        if len(self.current_window) > self.window_size:\n",
    "            self.current_window.pop(0)\n",
    "        if len(self.reference_window) < self.window_size:\n",
    "            self.reference_window.append(value)\n",
    "            return False\n",
    "        if len(self.current_window) == self.window_size:\n",
    "            stat, p_value = self._permutation_test()\n",
    "            self.drift_detected = p_value < self.alpha\n",
    "            if self.drift_detected:\n",
    "                self.reference_window = self.current_window.copy()\n",
    "        return self.drift_detected\n",
    "\n",
    "    def _permutation_test(self):\n",
    "        ref = np.array(self.reference_window)\n",
    "        curr = np.array(self.current_window)\n",
    "        observed_diff = np.abs(np.mean(ref) - np.mean(curr))\n",
    "        combined = np.concatenate([ref, curr])\n",
    "        perm_diffs = []\n",
    "        for _ in range(self.n_permutations):\n",
    "            np.random.shuffle(combined)\n",
    "            perm_ref = combined[: self.window_size]\n",
    "            perm_curr = combined[self.window_size :]\n",
    "            perm_diffs.append(np.abs(np.mean(perm_ref) - np.mean(perm_curr)))\n",
    "        p_value = np.sum(np.array(perm_diffs) >= observed_diff) / self.n_permutations\n",
    "        return observed_diff, p_value\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Drift detection + AL pipeline\n",
    "# -----------------------------\n",
    "def detect_natural_drift(data_file, data_type):\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(f\"{data_type.capitalize()} dataset loaded: {len(df)} rows\")\n",
    "    print(f\"Available columns: {list(df.columns)[:20]} ...\")\n",
    "\n",
    "    # Dynamically select numeric features\n",
    "    exclude_cols = [\"subject\", \"sessionIndex\", \"rep\"]\n",
    "    features = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "    if not features:\n",
    "        raise ValueError(f\"No valid features found in {data_type} dataset!\")\n",
    "\n",
    "    X = df[features].dropna().values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Base anomaly models (global, not per-feature)\n",
    "    ocsvm = OneClassSVM(nu=0.01, gamma=\"scale\")\n",
    "    iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "    ocsvm.fit(X_scaled[:1000])\n",
    "    iso.fit(X_scaled[:1000])\n",
    "\n",
    "    # Initialize drift detectors per feature\n",
    "    detectors = {}\n",
    "    for f in features:\n",
    "        detectors[f\"ADWIN_{f}\"] = ADWIN(delta=0.01)\n",
    "        detectors[f\"PH_{f}\"] = PageHinkley(threshold=10, alpha=0.01)\n",
    "        detectors[f\"KSWIN_{f}\"] = KSWIN(alpha=0.1, window_size=100)\n",
    "        detectors[f\"PCDM_{f}\"] = PCDM(alpha=0.05)\n",
    "    detectors[\"DDM_ERR\"] = DDM()  # error-based only once\n",
    "\n",
    "    drift_points = {name: [] for name in detectors}\n",
    "    AL_values = []\n",
    "\n",
    "    in_recovery = False\n",
    "    recovery_start = None\n",
    "\n",
    "    for i, x in enumerate(X_scaled):\n",
    "        err_ocsvm = 1 if ocsvm.predict([x])[0] == -1 else 0\n",
    "        err_iso = 1 if iso.predict([x])[0] == -1 else 0\n",
    "\n",
    "        drift_detected = False\n",
    "        for j, f in enumerate(features):\n",
    "            val = x[j]\n",
    "            for name, det in detectors.items():\n",
    "                if name == \"DDM_ERR\":\n",
    "                    drift = det.add_element(err_ocsvm or err_iso)\n",
    "                elif name.startswith(\"PCDM\") and name.endswith(f):\n",
    "                    drift = det.add_element(val)\n",
    "                elif name.startswith(\"ADWIN\") and name.endswith(f):\n",
    "                    drift = det.update(val)\n",
    "                elif name.startswith(\"PH\") and name.endswith(f):\n",
    "                    drift = det.update(val)\n",
    "                elif name.startswith(\"KSWIN\") and name.endswith(f):\n",
    "                    drift = det.update(val)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if drift:\n",
    "                    drift_detected = True\n",
    "                    drift_points[name].append(i)\n",
    "\n",
    "        # Adaptation Latency\n",
    "        if drift_detected and not in_recovery:\n",
    "            in_recovery = True\n",
    "            recovery_start = i\n",
    "\n",
    "        if in_recovery:\n",
    "            if i - recovery_start > 200:\n",
    "                recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled[i-200:i]]\n",
    "                if np.mean(recent_errs) < 0.05:\n",
    "                    AL = i - recovery_start\n",
    "                    AL_values.append(AL)\n",
    "                    in_recovery = False\n",
    "\n",
    "    # -----------------------------\n",
    "    # Summary per feature\n",
    "    # -----------------------------\n",
    "    print(\"\\n=== SUMMARY ===============================\")\n",
    "    print(f\"Data type: {data_type}\")\n",
    "    print(f\"Samples: {len(X_scaled)} | Features: {len(features)}\")\n",
    "    print(\"------------------------------------------\")\n",
    "    for det, pts in drift_points.items():\n",
    "        if len(pts) > 0:\n",
    "            print(f\"  {det:15}: {len(pts)} detections\")\n",
    "    if AL_values:\n",
    "        print(f\"\\nAdaptation Latency (mean over drifts): {np.mean(AL_values):.2f} samples\")\n",
    "    else:\n",
    "        print(\"\\nNo AL measured (no recovery detected).\")\n",
    "    print(\"==========================================\\n\")\n",
    "\n",
    "    return drift_points, AL_values\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run for CMU keystroke dataset\n",
    "# -----------------------------\n",
    "datasets = {\n",
    "    \"keystroke\": \"/Users/festusedward-n/Documents/Datasets/DSL-StrongPasswordData 2.csv\"\n",
    "}\n",
    "\n",
    "for dtype, path in datasets.items():\n",
    "    drift_points, AL = detect_natural_drift(path, dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac6b39-38a7-4d90-9ba7-14ed8f405bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f77256f-cb7b-4376-9172-cc10e40b304e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keystroke dataset loaded: 20400 rows\n",
      "Available columns: ['subject', 'sessionIndex', 'rep', 'H.period', 'DD.period.t', 'UD.period.t', 'H.t', 'DD.t.i', 'UD.t.i', 'H.i', 'DD.i.e', 'UD.i.e', 'H.e', 'DD.e.five', 'UD.e.five', 'H.five', 'DD.five.Shift.r', 'UD.five.Shift.r', 'H.Shift.r', 'DD.Shift.r.o'] ...\n",
      "\n",
      "=== SUMMARY ===============================\n",
      "Data type: keystroke\n",
      "Samples: 20400 | Features: ['H.period', 'DD.period.t', 'UD.period.t', 'H.t', 'DD.t.i', 'UD.t.i', 'H.i', 'DD.i.e', 'UD.i.e', 'H.e'] ...\n",
      "------------------------------------------\n",
      "  ADWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  PH_PC       : 0 detections (FAR=0.00%)\n",
      "  KSWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  DDM_ERR     : 17663 detections (FAR=86.58%)\n",
      "  PCDM_PC     : 604 detections (FAR=2.96%)\n",
      "  ADWIN_OCSVM : 0 detections (FAR=0.00%)\n",
      "  ADWIN_ISO   : 0 detections (FAR=0.00%)\n",
      "\n",
      "Adaptation Latency (AL): 3248.80 samples\n",
      "Detection Delay Accuracy (DDA): 3248.80 samples\n",
      "Recovery Accuracy (RA): 99.65%\n",
      "==========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from river.drift import ADWIN, PageHinkley, KSWIN\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# -----------------------------\n",
    "# Custom detectors\n",
    "# -----------------------------\n",
    "class DDM:\n",
    "    def __init__(self, min_num_instances=30, warning_level=2.0, drift_level=3.0):\n",
    "        self.min_num_instances = min_num_instances\n",
    "        self.warning_level = warning_level\n",
    "        self.drift_level = drift_level\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "        self.n = 0\n",
    "        self.drift_detected = False\n",
    "        self.mean_min = float('inf')\n",
    "        self.std_min = float('inf')\n",
    "\n",
    "    def add_element(self, error):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = error\n",
    "            self.std = 0.0\n",
    "        else:\n",
    "            old_mean = self.mean\n",
    "            self.mean += (error - old_mean) / self.n\n",
    "            self.std = np.sqrt(\n",
    "                (self.std**2 * (self.n - 1) + (error - self.mean) * (error - old_mean)) / self.n\n",
    "            )\n",
    "        if self.n >= self.min_num_instances:\n",
    "            if self.mean + self.std > self.mean_min + self.drift_level * self.std_min:\n",
    "                self.drift_detected = True\n",
    "            else:\n",
    "                self.drift_detected = False\n",
    "            if not self.drift_detected:\n",
    "                self.mean_min = min(self.mean_min, self.mean)\n",
    "                self.std_min = min(self.std_min, self.std)\n",
    "        return self.drift_detected\n",
    "\n",
    "\n",
    "class PCDM:\n",
    "    def __init__(self, window_size=50, n_permutations=50, alpha=0.05):\n",
    "        self.window_size = window_size\n",
    "        self.n_permutations = n_permutations\n",
    "        self.alpha = alpha\n",
    "        self.reference_window = []\n",
    "        self.current_window = []\n",
    "        self.drift_detected = False\n",
    "\n",
    "    def add_element(self, value):\n",
    "        self.current_window.append(value)\n",
    "        if len(self.current_window) > self.window_size:\n",
    "            self.current_window.pop(0)\n",
    "        if len(self.reference_window) < self.window_size:\n",
    "            self.reference_window.append(value)\n",
    "            return False\n",
    "        if len(self.current_window) == self.window_size:\n",
    "            stat, p_value = self._permutation_test()\n",
    "            self.drift_detected = p_value < self.alpha\n",
    "            if self.drift_detected:\n",
    "                self.reference_window = self.current_window.copy()\n",
    "        return self.drift_detected\n",
    "\n",
    "    def _permutation_test(self):\n",
    "        ref = np.array(self.reference_window)\n",
    "        curr = np.array(self.current_window)\n",
    "        observed_diff = np.abs(np.mean(ref) - np.mean(curr))\n",
    "        combined = np.concatenate([ref, curr])\n",
    "        perm_diffs = []\n",
    "        for _ in range(self.n_permutations):\n",
    "            np.random.shuffle(combined)\n",
    "            perm_ref = combined[: self.window_size]\n",
    "            perm_curr = combined[self.window_size :]\n",
    "            perm_diffs.append(np.abs(np.mean(perm_ref) - np.mean(perm_curr)))\n",
    "        p_value = np.sum(np.array(perm_diffs) >= observed_diff) / self.n_permutations\n",
    "        return observed_diff, p_value\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Drift detection + AL pipeline\n",
    "# -----------------------------\n",
    "def detect_natural_drift(data_file, data_type):\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(f\"{data_type.capitalize()} dataset loaded: {len(df)} rows\")\n",
    "    print(f\"Available columns: {list(df.columns)[:20]} ...\")\n",
    "\n",
    "    # Select numeric features\n",
    "    exclude_cols = [\"subject\", \"sessionIndex\", \"rep\"]\n",
    "    features = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "    if not features:\n",
    "        raise ValueError(f\"No valid features found in {data_type} dataset!\")\n",
    "\n",
    "    X = df[features].dropna().values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Base anomaly models\n",
    "    ocsvm = OneClassSVM(nu=0.01, gamma=\"scale\")\n",
    "    iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "    ocsvm.fit(X_scaled[:1000])\n",
    "    iso.fit(X_scaled[:1000])\n",
    "\n",
    "    # Initialize detectors\n",
    "    detectors = {\n",
    "        \"ADWIN_PC\": ADWIN(delta=0.01),\n",
    "        \"PH_PC\": PageHinkley(threshold=10, alpha=0.01),\n",
    "        \"KSWIN_PC\": KSWIN(alpha=0.1, window_size=100),\n",
    "        \"DDM_ERR\": DDM(),\n",
    "        \"PCDM_PC\": PCDM(alpha=0.05),\n",
    "        \"ADWIN_OCSVM\": ADWIN(delta=0.01),\n",
    "        \"ADWIN_ISO\": ADWIN(delta=0.01),\n",
    "    }\n",
    "\n",
    "    drift_points = {name: [] for name in detectors}\n",
    "    AL_values, DDA_values, RA_values = [], [], []\n",
    "    FAR_values = {}\n",
    "\n",
    "    in_recovery = False\n",
    "    recovery_start = None\n",
    "    baseline_acc = None\n",
    "\n",
    "    for i, x in enumerate(X_scaled):\n",
    "        x_val = np.mean(x)\n",
    "        err_ocsvm = 1 if ocsvm.predict([x])[0] == -1 else 0\n",
    "        err_iso = 1 if iso.predict([x])[0] == -1 else 0\n",
    "\n",
    "        drift_detected = False\n",
    "        for name, det in detectors.items():\n",
    "            if name == \"DDM_ERR\":\n",
    "                drift = det.add_element(err_ocsvm or err_iso)\n",
    "            elif name == \"PCDM_PC\":\n",
    "                drift = det.add_element(x_val)\n",
    "            elif name == \"ADWIN_OCSVM\":\n",
    "                drift = det.update(err_ocsvm)\n",
    "            elif name == \"ADWIN_ISO\":\n",
    "                drift = det.update(err_iso)\n",
    "            else:\n",
    "                drift = det.update(x_val)\n",
    "            if drift:\n",
    "                drift_detected = True\n",
    "                drift_points[name].append(i)\n",
    "\n",
    "        # Adaptation Latency\n",
    "        if drift_detected and not in_recovery:\n",
    "            in_recovery = True\n",
    "            recovery_start = i\n",
    "            # Save baseline accuracy before drift\n",
    "            recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled[max(0, i-200):i]]\n",
    "            baseline_acc = 1 - np.mean(recent_errs) if len(recent_errs) > 0 else None\n",
    "\n",
    "        if in_recovery:\n",
    "            if i - recovery_start > 200:\n",
    "                recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled[i-200:i]]\n",
    "                if np.mean(recent_errs) < 0.05:\n",
    "                    AL = i - recovery_start\n",
    "                    AL_values.append(AL)\n",
    "\n",
    "                    # DDA = detection delay (here approximated as AL itself)\n",
    "                    DDA_values.append(AL)\n",
    "\n",
    "                    # RA = recovery accuracy compared to baseline\n",
    "                    if baseline_acc is not None:\n",
    "                        rec_acc = 1 - np.mean(recent_errs)\n",
    "                        RA_values.append(rec_acc / baseline_acc * 100)\n",
    "\n",
    "                    in_recovery = False\n",
    "\n",
    "    # FAR = detections / total samples\n",
    "    total_samples = len(X_scaled)\n",
    "    for det, pts in drift_points.items():\n",
    "        FAR_values[det] = len(pts) / total_samples * 100\n",
    "\n",
    "    print(\"\\n=== SUMMARY ===============================\")\n",
    "    print(f\"Data type: {data_type}\")\n",
    "    print(f\"Samples: {len(X_scaled)} | Features: {features[:10]} ...\")\n",
    "    print(\"------------------------------------------\")\n",
    "    for det, pts in drift_points.items():\n",
    "        print(f\"  {det:12}: {len(pts)} detections (FAR={FAR_values[det]:.2f}%)\")\n",
    "\n",
    "    if AL_values:\n",
    "        print(f\"\\nAdaptation Latency (AL): {np.mean(AL_values):.2f} samples\")\n",
    "        print(f\"Detection Delay Accuracy (DDA): {np.mean(DDA_values):.2f} samples\")\n",
    "        print(f\"Recovery Accuracy (RA): {np.mean(RA_values):.2f}%\")\n",
    "    else:\n",
    "        print(\"\\nNo AL/DDA/RA measured (no recovery detected).\")\n",
    "    print(\"==========================================\\n\")\n",
    "\n",
    "    return drift_points, AL_values, DDA_values, RA_values, FAR_values\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run for CMU keystroke dataset\n",
    "# -----------------------------\n",
    "datasets = {\n",
    "    \"keystroke\": \"/Users/festusedward-n/Documents/Datasets/DSL-StrongPasswordData 2.csv\"\n",
    "}\n",
    "\n",
    "for dtype, path in datasets.items():\n",
    "    drift_points, AL, DDA, RA, FAR = detect_natural_drift(path, dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc4f4a0-081b-4c76-9a29-e4a005a90a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e35508-feae-4ab2-847b-a6d18b648ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40dc02ff-fac5-49ab-8f1a-b4977eff2455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keystroke dataset loaded: 20400 rows\n",
      "Available columns: ['subject', 'sessionIndex', 'rep', 'H.period', 'DD.period.t', 'UD.period.t', 'H.t', 'DD.t.i', 'UD.t.i', 'H.i', 'DD.i.e', 'UD.i.e', 'H.e', 'DD.e.five', 'UD.e.five', 'H.five', 'DD.five.Shift.r', 'UD.five.Shift.r', 'H.Shift.r', 'DD.Shift.r.o'] ...\n",
      "\n",
      "=== SUMMARY ===============================\n",
      "Data type: keystroke\n",
      "Samples: 20400 | Features: 31\n",
      "------------------------------------------\n",
      "  ADWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  PH_PC       : 0 detections (FAR=0.00%)\n",
      "  KSWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  DDM_ERR     : 15580 detections (FAR=76.37%)\n",
      "  PCDM_PC     : 628 detections (FAR=3.08%)\n",
      "  ADWIN_OCSVM : 0 detections (FAR=0.00%)\n",
      "  ADWIN_ISO   : 0 detections (FAR=0.00%)\n",
      "\n",
      "Adaptation Latency (AL): 718.27 samples\n",
      "Detection Delay Accuracy (DDA): 718.27 samples\n",
      "Recovery Accuracy (RA): 99.88%\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 207\u001b[0m\n\u001b[1;32m    205\u001b[0m drift_points, AL, DDA, RA, FAR \u001b[38;5;241m=\u001b[39m detect_natural_drift(path, dtype)\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# Average Error Rate (AER) from OCSVM\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m all_errs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ocsvm\u001b[38;5;241m.\u001b[39mpredict([z])[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m z \u001b[38;5;129;01min\u001b[39;00m \u001b[43mX_scaled\u001b[49m]\n\u001b[1;32m    208\u001b[0m AER \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(all_errs) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Drift Density (DD) using PCDM\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from river.drift import ADWIN, PageHinkley, KSWIN\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# -----------------------------\n",
    "# Custom detectors\n",
    "# -----------------------------\n",
    "class DDM:\n",
    "    def __init__(self, min_num_instances=30, warning_level=2.0, drift_level=3.0):\n",
    "        self.min_num_instances = min_num_instances\n",
    "        self.warning_level = warning_level\n",
    "        self.drift_level = drift_level\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "        self.n = 0\n",
    "        self.drift_detected = False\n",
    "        self.mean_min = float('inf')\n",
    "        self.std_min = float('inf')\n",
    "\n",
    "    def add_element(self, error):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = error\n",
    "            self.std = 0.0\n",
    "        else:\n",
    "            old_mean = self.mean\n",
    "            self.mean += (error - old_mean) / self.n\n",
    "            self.std = np.sqrt(\n",
    "                (self.std**2 * (self.n - 1) + (error - self.mean) * (error - old_mean)) / self.n\n",
    "            )\n",
    "        if self.n >= self.min_num_instances:\n",
    "            if self.mean + self.std > self.mean_min + self.drift_level * self.std_min:\n",
    "                self.drift_detected = True\n",
    "            else:\n",
    "                self.drift_detected = False\n",
    "            if not self.drift_detected:\n",
    "                self.mean_min = min(self.mean_min, self.mean)\n",
    "                self.std_min = min(self.std_min, self.std)\n",
    "        return self.drift_detected\n",
    "\n",
    "\n",
    "class PCDM:\n",
    "    def __init__(self, window_size=50, n_permutations=50, alpha=0.05):\n",
    "        self.window_size = window_size\n",
    "        self.n_permutations = n_permutations\n",
    "        self.alpha = alpha\n",
    "        self.reference_window = []\n",
    "        self.current_window = []\n",
    "        self.drift_detected = False\n",
    "\n",
    "    def add_element(self, value):\n",
    "        self.current_window.append(value)\n",
    "        if len(self.current_window) > self.window_size:\n",
    "            self.current_window.pop(0)\n",
    "        if len(self.reference_window) < self.window_size:\n",
    "            self.reference_window.append(value)\n",
    "            return False\n",
    "        if len(self.current_window) == self.window_size:\n",
    "            stat, p_value = self._permutation_test()\n",
    "            self.drift_detected = p_value < self.alpha\n",
    "            if self.drift_detected:\n",
    "                self.reference_window = self.current_window.copy()\n",
    "        return self.drift_detected\n",
    "\n",
    "    def _permutation_test(self):\n",
    "        ref = np.array(self.reference_window)\n",
    "        curr = np.array(self.current_window)\n",
    "        observed_diff = np.abs(np.mean(ref) - np.mean(curr))\n",
    "        combined = np.concatenate([ref, curr])\n",
    "        perm_diffs = []\n",
    "        for _ in range(self.n_permutations):\n",
    "            np.random.shuffle(combined)\n",
    "            perm_ref = combined[: self.window_size]\n",
    "            perm_curr = combined[self.window_size:]\n",
    "            perm_diffs.append(np.abs(np.mean(perm_ref) - np.mean(perm_curr)))\n",
    "        p_value = np.sum(np.array(perm_diffs) >= observed_diff) / self.n_permutations\n",
    "        return observed_diff, p_value\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Drift detection + metrics pipeline\n",
    "# -----------------------------\n",
    "def detect_natural_drift(data_file, data_type):\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(f\"{data_type.capitalize()} dataset loaded: {len(df)} rows\")\n",
    "    print(f\"Available columns: {list(df.columns)[:20]} ...\")\n",
    "\n",
    "    # For CMU: drop IDs and keep numeric timing features (H., DD., UD.)\n",
    "    exclude_cols = [\"subject\", \"sessionIndex\", \"rep\"]\n",
    "    features = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "    if not features:\n",
    "        raise ValueError(f\"No valid features found in {data_type} dataset!\")\n",
    "\n",
    "    X = df[features].dropna().values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Base anomaly models\n",
    "    ocsvm = OneClassSVM(nu=0.05, gamma=\"scale\")   # more relaxed than 0.01\n",
    "    iso = IsolationForest(contamination=0.05, random_state=42)  # more realistic contamination\n",
    "    ocsvm.fit(X_scaled[:2000])  # bigger reference set\n",
    "    iso.fit(X_scaled[:2000])\n",
    "\n",
    "    # Initialize detectors\n",
    "    detectors = {\n",
    "        \"ADWIN_PC\": ADWIN(delta=0.01),\n",
    "        \"PH_PC\": PageHinkley(threshold=10, alpha=0.01),\n",
    "        \"KSWIN_PC\": KSWIN(alpha=0.1, window_size=100),\n",
    "        \"DDM_ERR\": DDM(),\n",
    "        \"PCDM_PC\": PCDM(alpha=0.05),\n",
    "        \"ADWIN_OCSVM\": ADWIN(delta=0.01),\n",
    "        \"ADWIN_ISO\": ADWIN(delta=0.01),\n",
    "    }\n",
    "\n",
    "    drift_points = {name: [] for name in detectors}\n",
    "    AL_values, DDA_values, RA_values = [], [], []\n",
    "    FAR_values = {}\n",
    "\n",
    "    in_recovery = False\n",
    "    recovery_start = None\n",
    "    baseline_acc = None\n",
    "\n",
    "    for i, x in enumerate(X_scaled):\n",
    "        x_val = np.mean(x)\n",
    "        err_ocsvm = 1 if ocsvm.predict([x])[0] == -1 else 0\n",
    "        err_iso = 1 if iso.predict([x])[0] == -1 else 0\n",
    "\n",
    "        drift_detected = False\n",
    "        for name, det in detectors.items():\n",
    "            if name == \"DDM_ERR\":\n",
    "                drift = det.add_element(err_ocsvm or err_iso)\n",
    "            elif name == \"PCDM_PC\":\n",
    "                drift = det.add_element(x_val)\n",
    "            elif name == \"ADWIN_OCSVM\":\n",
    "                drift = det.update(err_ocsvm)\n",
    "            elif name == \"ADWIN_ISO\":\n",
    "                drift = det.update(err_iso)\n",
    "            else:\n",
    "                drift = det.update(x_val)\n",
    "            if drift:\n",
    "                drift_detected = True\n",
    "                drift_points[name].append(i)\n",
    "\n",
    "        # Adaptation Latency\n",
    "        if drift_detected and not in_recovery:\n",
    "            in_recovery = True\n",
    "            recovery_start = i\n",
    "            # Save baseline accuracy before drift\n",
    "            recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled[max(0, i-200):i]]\n",
    "            baseline_acc = 1 - np.mean(recent_errs) if len(recent_errs) > 0 else None\n",
    "\n",
    "        if in_recovery:\n",
    "            if i - recovery_start > 200:\n",
    "                recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled[i-200:i]]\n",
    "                if np.mean(recent_errs) < 0.1:  # allow up to 10% anomaly rate\n",
    "                    AL = i - recovery_start\n",
    "                    AL_values.append(AL)\n",
    "\n",
    "                    # DDA ~ AL\n",
    "                    DDA_values.append(AL)\n",
    "\n",
    "                    # RA = recovery accuracy compared to baseline\n",
    "                    if baseline_acc is not None:\n",
    "                        rec_acc = 1 - np.mean(recent_errs)\n",
    "                        RA_values.append(rec_acc / baseline_acc * 100)\n",
    "\n",
    "                    in_recovery = False\n",
    "\n",
    "    # FAR = detections / total samples\n",
    "    total_samples = len(X_scaled)\n",
    "    for det, pts in drift_points.items():\n",
    "        FAR_values[det] = len(pts) / total_samples * 100\n",
    "\n",
    "    print(\"\\n=== SUMMARY ===============================\")\n",
    "    print(f\"Data type: {data_type}\")\n",
    "    print(f\"Samples: {len(X_scaled)} | Features: {len(features)}\")\n",
    "    print(\"------------------------------------------\")\n",
    "    for det, pts in drift_points.items():\n",
    "        print(f\"  {det:12}: {len(pts)} detections (FAR={FAR_values[det]:.2f}%)\")\n",
    "\n",
    "    if AL_values:\n",
    "        print(f\"\\nAdaptation Latency (AL): {np.mean(AL_values):.2f} samples\")\n",
    "        print(f\"Detection Delay Accuracy (DDA): {np.mean(DDA_values):.2f} samples\")\n",
    "        print(f\"Recovery Accuracy (RA): {np.mean(RA_values):.2f}%\")\n",
    "    else:\n",
    "        print(\"\\nNo AL/DDA/RA measured (no recovery detected).\")\n",
    "    print(\"==========================================\\n\")\n",
    "\n",
    "    return drift_points, AL_values, DDA_values, RA_values, FAR_values\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run for CMU DSL dataset\n",
    "# -----------------------------\n",
    "datasets = {\n",
    "    \"keystroke\": \"/Users/festusedward-n/Documents/Datasets/DSL-StrongPasswordData 2.csv\"\n",
    "}\n",
    "\n",
    "for dtype, path in datasets.items():\n",
    "    drift_points, AL, DDA, RA, FAR = detect_natural_drift(path, dtype)\n",
    "        # Average Error Rate (AER) from OCSVM\n",
    "    all_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled]\n",
    "    AER = np.mean(all_errs) * 100\n",
    "\n",
    "    # Drift Density (DD) using PCDM\n",
    "    DD = len(drift_points[\"PCDM_PC\"]) / total_samples * 100\n",
    "\n",
    "    print(f\"Average Error Rate (AER): {AER:.2f}%\")\n",
    "    print(f\"Drift Density (DD): {DD:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e0c77-0406-4be7-94fa-29433f77c651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f5ed6c-175a-40d2-8350-197858e9eb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f05112e-8a7a-47cd-a1a7-47b1f1c80432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keystroke dataset loaded: 20400 rows\n",
      "Available columns: ['subject', 'sessionIndex', 'rep', 'H.period', 'DD.period.t', 'UD.period.t', 'H.t', 'DD.t.i', 'UD.t.i', 'H.i', 'DD.i.e', 'UD.i.e', 'H.e', 'DD.e.five', 'UD.e.five', 'H.five', 'DD.five.Shift.r', 'UD.five.Shift.r', 'H.Shift.r', 'DD.Shift.r.o'] ...\n",
      "\n",
      "=== SUMMARY ===============================\n",
      "Data type: keystroke\n",
      "Samples: 20400 | Features: ['H.period', 'DD.period.t', 'UD.period.t', 'H.t', 'DD.t.i', 'UD.t.i', 'H.i', 'DD.i.e', 'UD.i.e', 'H.e'] ...\n",
      "------------------------------------------\n",
      "  ADWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  PH_PC       : 0 detections (FAR=0.00%)\n",
      "  KSWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  DDM_ERR     : 17663 detections (FAR=86.58%)\n",
      "  PCDM_PC     : 608 detections (FAR=2.98%)\n",
      "  ADWIN_OCSVM : 0 detections (FAR=0.00%)\n",
      "  ADWIN_ISO   : 0 detections (FAR=0.00%)\n",
      "\n",
      "Adaptation Latency (AL): 2346.71 samples\n",
      "Detection Delay Accuracy (DDA): 2346.71 samples\n",
      "Recovery Accuracy (RA): 100.35%\n",
      "Average Error Rate (AER): 44.59%\n",
      "Drift Density (DD): 2.98%\n",
      "==========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from river.drift import ADWIN, PageHinkley, KSWIN\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# -----------------------------\n",
    "# Custom detectors\n",
    "# -----------------------------\n",
    "class DDM:\n",
    "    def __init__(self, min_num_instances=30, warning_level=2.0, drift_level=3.0):\n",
    "        self.min_num_instances = min_num_instances\n",
    "        self.warning_level = warning_level\n",
    "        self.drift_level = drift_level\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "        self.n = 0\n",
    "        self.drift_detected = False\n",
    "        self.mean_min = float('inf')\n",
    "        self.std_min = float('inf')\n",
    "\n",
    "    def add_element(self, error):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = error\n",
    "            self.std = 0.0\n",
    "        else:\n",
    "            old_mean = self.mean\n",
    "            self.mean += (error - old_mean) / self.n\n",
    "            self.std = np.sqrt(\n",
    "                (self.std**2 * (self.n - 1) + (error - self.mean) * (error - old_mean)) / self.n\n",
    "            )\n",
    "        if self.n >= self.min_num_instances:\n",
    "            if self.mean + self.std > self.mean_min + self.drift_level * self.std_min:\n",
    "                self.drift_detected = True\n",
    "            else:\n",
    "                self.drift_detected = False\n",
    "            if not self.drift_detected:\n",
    "                self.mean_min = min(self.mean_min, self.mean)\n",
    "                self.std_min = min(self.std_min, self.std)\n",
    "        return self.drift_detected\n",
    "\n",
    "\n",
    "class PCDM:\n",
    "    def __init__(self, window_size=50, n_permutations=50, alpha=0.05):\n",
    "        self.window_size = window_size\n",
    "        self.n_permutations = n_permutations\n",
    "        self.alpha = alpha\n",
    "        self.reference_window = []\n",
    "        self.current_window = []\n",
    "        self.drift_detected = False\n",
    "\n",
    "    def add_element(self, value):\n",
    "        self.current_window.append(value)\n",
    "        if len(self.current_window) > self.window_size:\n",
    "            self.current_window.pop(0)\n",
    "        if len(self.reference_window) < self.window_size:\n",
    "            self.reference_window.append(value)\n",
    "            return False\n",
    "        if len(self.current_window) == self.window_size:\n",
    "            stat, p_value = self._permutation_test()\n",
    "            self.drift_detected = p_value < self.alpha\n",
    "            if self.drift_detected:\n",
    "                self.reference_window = self.current_window.copy()\n",
    "        return self.drift_detected\n",
    "\n",
    "    def _permutation_test(self):\n",
    "        ref = np.array(self.reference_window)\n",
    "        curr = np.array(self.current_window)\n",
    "        observed_diff = np.abs(np.mean(ref) - np.mean(curr))\n",
    "        combined = np.concatenate([ref, curr])\n",
    "        perm_diffs = []\n",
    "        for _ in range(self.n_permutations):\n",
    "            np.random.shuffle(combined)\n",
    "            perm_ref = combined[: self.window_size]\n",
    "            perm_curr = combined[self.window_size :]\n",
    "            perm_diffs.append(np.abs(np.mean(perm_ref) - np.mean(perm_curr)))\n",
    "        p_value = np.sum(np.array(perm_diffs) >= observed_diff) / self.n_permutations\n",
    "        return observed_diff, p_value\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Drift detection + metrics pipeline\n",
    "# -----------------------------\n",
    "def detect_natural_drift(data_file, data_type):\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(f\"{data_type.capitalize()} dataset loaded: {len(df)} rows\")\n",
    "    print(f\"Available columns: {list(df.columns)[:20]} ...\")\n",
    "\n",
    "    # Select numeric features (exclude IDs/labels)\n",
    "    exclude_cols = [\"subject\", \"sessionIndex\", \"rep\"]\n",
    "    features = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "    if not features:\n",
    "        raise ValueError(f\"No valid features found in {data_type} dataset!\")\n",
    "\n",
    "    X = df[features].dropna().values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Base anomaly models\n",
    "    ocsvm = OneClassSVM(nu=0.01, gamma=\"scale\")\n",
    "    iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "    ocsvm.fit(X_scaled[:1000])\n",
    "    iso.fit(X_scaled[:1000])\n",
    "\n",
    "    # Initialize detectors\n",
    "    detectors = {\n",
    "        \"ADWIN_PC\": ADWIN(delta=0.01),\n",
    "        \"PH_PC\": PageHinkley(threshold=10, alpha=0.01),\n",
    "        \"KSWIN_PC\": KSWIN(alpha=0.1, window_size=100),\n",
    "        \"DDM_ERR\": DDM(),\n",
    "        \"PCDM_PC\": PCDM(alpha=0.05),\n",
    "        \"ADWIN_OCSVM\": ADWIN(delta=0.01),\n",
    "        \"ADWIN_ISO\": ADWIN(delta=0.01),\n",
    "    }\n",
    "\n",
    "    drift_points = {name: [] for name in detectors}\n",
    "    AL_values, DDA_values, RA_values = [], [], []\n",
    "    FAR_values = {}\n",
    "\n",
    "    in_recovery = False\n",
    "    recovery_start = None\n",
    "    baseline_acc = None\n",
    "\n",
    "    # Track errors for AER\n",
    "    all_errs = []\n",
    "\n",
    "    for i, x in enumerate(X_scaled):\n",
    "        x_val = np.mean(x)\n",
    "        err_ocsvm = 1 if ocsvm.predict([x])[0] == -1 else 0\n",
    "        err_iso = 1 if iso.predict([x])[0] == -1 else 0\n",
    "        all_errs.append(err_ocsvm or err_iso)\n",
    "\n",
    "        drift_detected = False\n",
    "        for name, det in detectors.items():\n",
    "            if name == \"DDM_ERR\":\n",
    "                drift = det.add_element(err_ocsvm or err_iso)\n",
    "            elif name == \"PCDM_PC\":\n",
    "                drift = det.add_element(x_val)\n",
    "            elif name == \"ADWIN_OCSVM\":\n",
    "                drift = det.update(err_ocsvm)\n",
    "            elif name == \"ADWIN_ISO\":\n",
    "                drift = det.update(err_iso)\n",
    "            else:\n",
    "                drift = det.update(x_val)\n",
    "            if drift:\n",
    "                drift_detected = True\n",
    "                drift_points[name].append(i)\n",
    "\n",
    "        # Adaptation metrics\n",
    "        if drift_detected and not in_recovery:\n",
    "            in_recovery = True\n",
    "            recovery_start = i\n",
    "            # Baseline accuracy before drift\n",
    "            recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0\n",
    "                           for z in X_scaled[max(0, i-200):i]]\n",
    "            baseline_acc = 1 - np.mean(recent_errs) if len(recent_errs) > 0 else None\n",
    "\n",
    "        if in_recovery:\n",
    "            if i - recovery_start > 200:\n",
    "                recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0\n",
    "                               for z in X_scaled[i-200:i]]\n",
    "                if np.mean(recent_errs) < 0.05:\n",
    "                    AL = i - recovery_start\n",
    "                    AL_values.append(AL)\n",
    "                    DDA_values.append(AL)  # here approximated as AL\n",
    "                    if baseline_acc is not None:\n",
    "                        rec_acc = 1 - np.mean(recent_errs)\n",
    "                        RA_values.append(rec_acc / baseline_acc * 100)\n",
    "                    in_recovery = False\n",
    "\n",
    "    # FAR = detections / total samples\n",
    "    total_samples = len(X_scaled)\n",
    "    for det, pts in drift_points.items():\n",
    "        FAR_values[det] = len(pts) / total_samples * 100\n",
    "\n",
    "    # Average Error Rate (AER)\n",
    "    AER = np.mean(all_errs) * 100\n",
    "\n",
    "    # Drift Density (DD) from PCDM\n",
    "    DD = len(drift_points[\"PCDM_PC\"]) / total_samples * 100\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n=== SUMMARY ===============================\")\n",
    "    print(f\"Data type: {data_type}\")\n",
    "    print(f\"Samples: {len(X_scaled)} | Features: {features[:10]} ...\")\n",
    "    print(\"------------------------------------------\")\n",
    "    for det, pts in drift_points.items():\n",
    "        print(f\"  {det:12}: {len(pts)} detections (FAR={FAR_values[det]:.2f}%)\")\n",
    "\n",
    "    if AL_values:\n",
    "        print(f\"\\nAdaptation Latency (AL): {np.mean(AL_values):.2f} samples\")\n",
    "        print(f\"Detection Delay Accuracy (DDA): {np.mean(DDA_values):.2f} samples\")\n",
    "        print(f\"Recovery Accuracy (RA): {np.mean(RA_values):.2f}%\")\n",
    "    else:\n",
    "        print(\"\\nNo AL/DDA/RA measured (no recovery detected).\")\n",
    "\n",
    "    print(f\"Average Error Rate (AER): {AER:.2f}%\")\n",
    "    print(f\"Drift Density (DD): {DD:.2f}%\")\n",
    "    print(\"==========================================\\n\")\n",
    "\n",
    "    return drift_points, AL_values, DDA_values, RA_values, FAR_values, AER, DD\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run for CMU keystroke dataset\n",
    "# -----------------------------\n",
    "datasets = {\n",
    "    \"keystroke\": \"/Users/festusedward-n/Documents/Datasets/DSL-StrongPasswordData 2.csv\"\n",
    "}\n",
    "\n",
    "for dtype, path in datasets.items():\n",
    "    detect_natural_drift(path, dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f8cb4b-7cee-413d-87d6-26f28fae3621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74900404-6a28-4241-a7ca-0675db8c7813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1288d4b2-7ad0-40d4-8e72-75331b226d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4915886-c2b1-4be9-a499-16bfa9eac631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keystroke dataset loaded: 20400 rows\n",
      "Available columns: ['subject', 'sessionIndex', 'rep', 'H.period', 'DD.period.t', 'UD.period.t', 'H.t', 'DD.t.i', 'UD.t.i', 'H.i', 'DD.i.e', 'UD.i.e', 'H.e', 'DD.e.five', 'UD.e.five', 'H.five', 'DD.five.Shift.r', 'UD.five.Shift.r', 'H.Shift.r', 'DD.Shift.r.o'] ...\n",
      "\n",
      "=== SUMMARY ===============================\n",
      "Data type: keystroke\n",
      "Samples: 20400 | Features: ['H.period', 'DD.period.t', 'UD.period.t', 'H.t', 'DD.t.i', 'UD.t.i', 'H.i', 'DD.i.e', 'UD.i.e', 'H.e'] ...\n",
      "------------------------------------------\n",
      "  ADWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  PH_PC       : 0 detections (FAR=0.00%)\n",
      "  KSWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  DDM_ERR     : 17663 detections (FAR=86.58%)\n",
      "  PCDM_PC     : 584 detections (FAR=2.86%)\n",
      "  ADWIN_OCSVM : 0 detections (FAR=0.00%)\n",
      "  ADWIN_ISO   : 0 detections (FAR=0.00%)\n",
      "\n",
      "Adaptation Latency (AL): 2354.14 samples\n",
      "Detection Delay Accuracy (DDA): 2354.14 samples\n",
      "Recovery Accuracy (RA): 25.05%\n",
      "Average Error Rate (AER): 29.73%\n",
      "Drift Density (DD): 12.78%\n",
      "==========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from river.drift import ADWIN, PageHinkley, KSWIN\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# -----------------------------\n",
    "# Custom detectors\n",
    "# -----------------------------\n",
    "class DDM:\n",
    "    def __init__(self, min_num_instances=30, warning_level=2.0, drift_level=3.0):\n",
    "        self.min_num_instances = min_num_instances\n",
    "        self.warning_level = warning_level\n",
    "        self.drift_level = drift_level\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "        self.n = 0\n",
    "        self.drift_detected = False\n",
    "        self.mean_min = float('inf')\n",
    "        self.std_min = float('inf')\n",
    "\n",
    "    def add_element(self, error):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = error\n",
    "            self.std = 0.0\n",
    "        else:\n",
    "            old_mean = self.mean\n",
    "            self.mean += (error - old_mean) / self.n\n",
    "            self.std = np.sqrt(\n",
    "                (self.std**2 * (self.n - 1) + (error - self.mean) * (error - old_mean)) / self.n\n",
    "            )\n",
    "        if self.n >= self.min_num_instances:\n",
    "            if self.mean + self.std > self.mean_min + self.drift_level * self.std_min:\n",
    "                self.drift_detected = True\n",
    "            else:\n",
    "                self.drift_detected = False\n",
    "            if not self.drift_detected:\n",
    "                self.mean_min = min(self.mean_min, self.mean)\n",
    "                self.std_min = min(self.std_min, self.std)\n",
    "        return self.drift_detected\n",
    "\n",
    "\n",
    "class PCDM:\n",
    "    def __init__(self, window_size=50, n_permutations=50, alpha=0.05):\n",
    "        self.window_size = window_size\n",
    "        self.n_permutations = n_permutations\n",
    "        self.alpha = alpha\n",
    "        self.reference_window = []\n",
    "        self.current_window = []\n",
    "        self.drift_detected = False\n",
    "\n",
    "    def add_element(self, value):\n",
    "        self.current_window.append(value)\n",
    "        if len(self.current_window) > self.window_size:\n",
    "            self.current_window.pop(0)\n",
    "        if len(self.reference_window) < self.window_size:\n",
    "            self.reference_window.append(value)\n",
    "            return False\n",
    "        if len(self.current_window) == self.window_size:\n",
    "            stat, p_value = self._permutation_test()\n",
    "            self.drift_detected = p_value < self.alpha\n",
    "            if self.drift_detected:\n",
    "                self.reference_window = self.current_window.copy()\n",
    "        return self.drift_detected\n",
    "\n",
    "    def _permutation_test(self):\n",
    "        ref = np.array(self.reference_window)\n",
    "        curr = np.array(self.current_window)\n",
    "        observed_diff = np.abs(np.mean(ref) - np.mean(curr))\n",
    "        combined = np.concatenate([ref, curr])\n",
    "        perm_diffs = []\n",
    "        for _ in range(self.n_permutations):\n",
    "            np.random.shuffle(combined)\n",
    "            perm_ref = combined[: self.window_size]\n",
    "            perm_curr = combined[self.window_size :]\n",
    "            perm_diffs.append(np.abs(np.mean(perm_ref) - np.mean(perm_curr)))\n",
    "        p_value = np.sum(np.array(perm_diffs) >= observed_diff) / self.n_permutations\n",
    "        return observed_diff, p_value\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Drift detection + metrics\n",
    "# -----------------------------\n",
    "def detect_natural_drift(data_file, data_type):\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(f\"{data_type.capitalize()} dataset loaded: {len(df)} rows\")\n",
    "    print(f\"Available columns: {list(df.columns)[:20]} ...\")\n",
    "\n",
    "    # Select numeric features (exclude identifiers)\n",
    "    exclude_cols = [\"subject\", \"sessionIndex\", \"rep\"]\n",
    "    features = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "    if not features:\n",
    "        raise ValueError(f\"No valid features found in {data_type} dataset!\")\n",
    "\n",
    "    X = df[features].dropna().values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Base anomaly models\n",
    "    ocsvm = OneClassSVM(nu=0.01, gamma=\"scale\")\n",
    "    iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "    ocsvm.fit(X_scaled[:1000])\n",
    "    iso.fit(X_scaled[:1000])\n",
    "\n",
    "    # Initialize detectors\n",
    "    detectors = {\n",
    "        \"ADWIN_PC\": ADWIN(delta=0.01),\n",
    "        \"PH_PC\": PageHinkley(threshold=10, alpha=0.01),\n",
    "        \"KSWIN_PC\": KSWIN(alpha=0.1, window_size=100),\n",
    "        \"DDM_ERR\": DDM(),\n",
    "        \"PCDM_PC\": PCDM(alpha=0.05),\n",
    "        \"ADWIN_OCSVM\": ADWIN(delta=0.01),\n",
    "        \"ADWIN_ISO\": ADWIN(delta=0.01),\n",
    "    }\n",
    "\n",
    "    drift_points = {name: [] for name in detectors}\n",
    "    AL_values, DDA_values, RA_values, AER_values = [], [], [], []\n",
    "    FAR_values = {}\n",
    "\n",
    "    in_recovery = False\n",
    "    recovery_start = None\n",
    "    baseline_acc = None\n",
    "\n",
    "    for i, x in enumerate(X_scaled):\n",
    "        x_val = np.mean(x)\n",
    "        err_ocsvm = 1 if ocsvm.predict([x])[0] == -1 else 0\n",
    "        err_iso = 1 if iso.predict([x])[0] == -1 else 0\n",
    "        avg_err = (err_ocsvm + err_iso) / 2.0\n",
    "        AER_values.append(avg_err)\n",
    "\n",
    "        drift_detected = False\n",
    "        for name, det in detectors.items():\n",
    "            if name == \"DDM_ERR\":\n",
    "                drift = det.add_element(err_ocsvm or err_iso)\n",
    "            elif name == \"PCDM_PC\":\n",
    "                drift = det.add_element(x_val)\n",
    "            elif name == \"ADWIN_OCSVM\":\n",
    "                drift = det.update(err_ocsvm)\n",
    "            elif name == \"ADWIN_ISO\":\n",
    "                drift = det.update(err_iso)\n",
    "            else:\n",
    "                drift = det.update(x_val)\n",
    "            if drift:\n",
    "                drift_detected = True\n",
    "                drift_points[name].append(i)\n",
    "\n",
    "        # Adaptation Latency\n",
    "        if drift_detected and not in_recovery:\n",
    "            in_recovery = True\n",
    "            recovery_start = i\n",
    "            recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled[max(0, i-200):i]]\n",
    "            baseline_acc = 1 - np.mean(recent_errs) if len(recent_errs) > 0 else None\n",
    "\n",
    "        if in_recovery:\n",
    "            if i - recovery_start > 200:\n",
    "                recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled[i-200:i]]\n",
    "                post_acc = 1 - np.mean(recent_errs)\n",
    "                baseline_err = 1 - baseline_acc if baseline_acc is not None else None\n",
    "                post_err = np.mean(recent_errs)\n",
    "\n",
    "                if post_err < 0.05:  # recovery condition\n",
    "                    AL = i - recovery_start\n",
    "                    AL_values.append(AL)\n",
    "                    DDA_values.append(AL)\n",
    "\n",
    "                    #  FIXED: Normalized RA (0100%)\n",
    "                    if baseline_err is not None and baseline_err > 0:\n",
    "                        RA = (1 - (post_err / baseline_err)) * 100\n",
    "                        RA_values.append(max(min(RA, 100), 0))  # bound between 0 and 100\n",
    "\n",
    "                    in_recovery = False\n",
    "\n",
    "    # FAR = detections / total samples\n",
    "    total_samples = len(X_scaled)\n",
    "    for det, pts in drift_points.items():\n",
    "        FAR_values[det] = len(pts) / total_samples * 100\n",
    "\n",
    "    print(\"\\n=== SUMMARY ===============================\")\n",
    "    print(f\"Data type: {data_type}\")\n",
    "    print(f\"Samples: {len(X_scaled)} | Features: {features[:10]} ...\")\n",
    "    print(\"------------------------------------------\")\n",
    "    for det, pts in drift_points.items():\n",
    "        print(f\"  {det:12}: {len(pts)} detections (FAR={FAR_values[det]:.2f}%)\")\n",
    "\n",
    "    if AL_values:\n",
    "        print(f\"\\nAdaptation Latency (AL): {np.mean(AL_values):.2f} samples\")\n",
    "        print(f\"Detection Delay Accuracy (DDA): {np.mean(DDA_values):.2f} samples\")\n",
    "        print(f\"Recovery Accuracy (RA): {np.mean(RA_values):.2f}%\")\n",
    "        print(f\"Average Error Rate (AER): {np.mean(AER_values)*100:.2f}%\")\n",
    "        print(f\"Drift Density (DD): {np.mean([len(pts) for pts in drift_points.values()])/total_samples*100:.2f}%\")\n",
    "    else:\n",
    "        print(\"\\nNo AL/DDA/RA measured (no recovery detected).\")\n",
    "    print(\"==========================================\\n\")\n",
    "\n",
    "    return drift_points, AL_values, DDA_values, RA_values, FAR_values\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run for CMU keystroke dataset\n",
    "# -----------------------------\n",
    "datasets = {\n",
    "    \"keystroke\": \"/Users/festusedward-n/Documents/Datasets/DSL-StrongPasswordData 2.csv\"\n",
    "}\n",
    "\n",
    "for dtype, path in datasets.items():\n",
    "    drift_points, AL, DDA, RA, FAR = detect_natural_drift(path, dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd226f53-0739-4d8c-9f66-ad37e9700bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dcc19a-a33c-4f79-8104-c507612636c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f113d5d1-4a89-4029-be75-de173d21dece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keystroke dataset loaded: 20400 rows\n",
      "Available columns: ['subject', 'sessionIndex', 'rep', 'H.period', 'DD.period.t', 'UD.period.t', 'H.t', 'DD.t.i', 'UD.t.i', 'H.i', 'DD.i.e', 'UD.i.e', 'H.e', 'DD.e.five', 'UD.e.five', 'H.five', 'DD.five.Shift.r', 'UD.five.Shift.r', 'H.Shift.r', 'DD.Shift.r.o'] ...\n",
      "\n",
      "=== SUMMARY ===============================\n",
      "Data type: keystroke\n",
      "Samples: 20400 | Features: ['H.period', 'DD.period.t', 'UD.period.t', 'H.t', 'DD.t.i', 'UD.t.i', 'H.i', 'DD.i.e', 'UD.i.e', 'H.e'] ...\n",
      "------------------------------------------\n",
      "  ADWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  PH_PC       : 0 detections (FAR=0.00%)\n",
      "  KSWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  DDM_ERR     : 17663 detections (FAR=86.58%)\n",
      "  PCDM_PC     : 613 detections (FAR=3.00%)\n",
      "  ADWIN_OCSVM : 0 detections (FAR=0.00%)\n",
      "  ADWIN_ISO   : 0 detections (FAR=0.00%)\n",
      "\n",
      "Adaptation Latency (AL): 2019.50 samples\n",
      "Detection Delay Accuracy (DDA): 2019.50 samples\n",
      "Recovery Accuracy (RA): 93.75%\n",
      "==========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from river.drift import ADWIN, PageHinkley, KSWIN\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# -----------------------------\n",
    "# Custom detectors\n",
    "# -----------------------------\n",
    "class DDM:\n",
    "    def __init__(self, min_num_instances=30, warning_level=2.0, drift_level=3.0):\n",
    "        self.min_num_instances = min_num_instances\n",
    "        self.warning_level = warning_level\n",
    "        self.drift_level = drift_level\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "        self.n = 0\n",
    "        self.drift_detected = False\n",
    "        self.mean_min = float('inf')\n",
    "        self.std_min = float('inf')\n",
    "\n",
    "    def add_element(self, error):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = error\n",
    "            self.std = 0.0\n",
    "        else:\n",
    "            old_mean = self.mean\n",
    "            self.mean += (error - old_mean) / self.n\n",
    "            self.std = np.sqrt(\n",
    "                (self.std**2 * (self.n - 1) + (error - self.mean) * (error - old_mean)) / self.n\n",
    "            )\n",
    "        if self.n >= self.min_num_instances:\n",
    "            if self.mean + self.std > self.mean_min + self.drift_level * self.std_min:\n",
    "                self.drift_detected = True\n",
    "            else:\n",
    "                self.drift_detected = False\n",
    "            if not self.drift_detected:\n",
    "                self.mean_min = min(self.mean_min, self.mean)\n",
    "                self.std_min = min(self.std_min, self.std)\n",
    "        return self.drift_detected\n",
    "\n",
    "\n",
    "class PCDM:\n",
    "    def __init__(self, window_size=50, n_permutations=50, alpha=0.05):\n",
    "        self.window_size = window_size\n",
    "        self.n_permutations = n_permutations\n",
    "        self.alpha = alpha\n",
    "        self.reference_window = []\n",
    "        self.current_window = []\n",
    "        self.drift_detected = False\n",
    "\n",
    "    def add_element(self, value):\n",
    "        self.current_window.append(value)\n",
    "        if len(self.current_window) > self.window_size:\n",
    "            self.current_window.pop(0)\n",
    "        if len(self.reference_window) < self.window_size:\n",
    "            self.reference_window.append(value)\n",
    "            return False\n",
    "        if len(self.current_window) == self.window_size:\n",
    "            stat, p_value = self._permutation_test()\n",
    "            self.drift_detected = p_value < self.alpha\n",
    "            if self.drift_detected:\n",
    "                self.reference_window = self.current_window.copy()\n",
    "        return self.drift_detected\n",
    "\n",
    "    def _permutation_test(self):\n",
    "        ref = np.array(self.reference_window)\n",
    "        curr = np.array(self.current_window)\n",
    "        observed_diff = np.abs(np.mean(ref) - np.mean(curr))\n",
    "        combined = np.concatenate([ref, curr])\n",
    "        perm_diffs = []\n",
    "        for _ in range(self.n_permutations):\n",
    "            np.random.shuffle(combined)\n",
    "            perm_ref = combined[: self.window_size]\n",
    "            perm_curr = combined[self.window_size :]\n",
    "            perm_diffs.append(np.abs(np.mean(perm_ref) - np.mean(perm_curr)))\n",
    "        p_value = np.sum(np.array(perm_diffs) >= observed_diff) / self.n_permutations\n",
    "        return observed_diff, p_value\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Drift detection + metrics pipeline\n",
    "# -----------------------------\n",
    "def detect_natural_drift(data_file, data_type):\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(f\"{data_type.capitalize()} dataset loaded: {len(df)} rows\")\n",
    "    print(f\"Available columns: {list(df.columns)[:20]} ...\")\n",
    "\n",
    "    # Select numeric features\n",
    "    exclude_cols = [\"subject\", \"sessionIndex\", \"rep\"]\n",
    "    features = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "    if not features:\n",
    "        raise ValueError(f\"No valid features found in {data_type} dataset!\")\n",
    "\n",
    "    X = df[features].dropna().values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Base anomaly models\n",
    "    ocsvm = OneClassSVM(nu=0.01, gamma=\"scale\")\n",
    "    iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "    ocsvm.fit(X_scaled[:1000])\n",
    "    iso.fit(X_scaled[:1000])\n",
    "\n",
    "    # Initialize detectors\n",
    "    detectors = {\n",
    "        \"ADWIN_PC\": ADWIN(delta=0.01),\n",
    "        \"PH_PC\": PageHinkley(threshold=10, alpha=0.01),\n",
    "        \"KSWIN_PC\": KSWIN(alpha=0.1, window_size=100),\n",
    "        \"DDM_ERR\": DDM(),\n",
    "        \"PCDM_PC\": PCDM(alpha=0.05),\n",
    "        \"ADWIN_OCSVM\": ADWIN(delta=0.01),\n",
    "        \"ADWIN_ISO\": ADWIN(delta=0.01),\n",
    "    }\n",
    "\n",
    "    drift_points = {name: [] for name in detectors}\n",
    "    AL_values, DDA_values, RA_values = [], [], []\n",
    "    FAR_values = {}\n",
    "\n",
    "    in_recovery = False\n",
    "    recovery_start = None\n",
    "\n",
    "    for i, x in enumerate(X_scaled):\n",
    "        x_val = np.mean(x)\n",
    "        err_ocsvm = 1 if ocsvm.predict([x])[0] == -1 else 0\n",
    "        err_iso = 1 if iso.predict([x])[0] == -1 else 0\n",
    "\n",
    "        drift_detected = False\n",
    "        for name, det in detectors.items():\n",
    "            if name == \"DDM_ERR\":\n",
    "                drift = det.add_element(err_ocsvm or err_iso)\n",
    "            elif name == \"PCDM_PC\":\n",
    "                drift = det.add_element(x_val)\n",
    "            elif name == \"ADWIN_OCSVM\":\n",
    "                drift = det.update(err_ocsvm)\n",
    "            elif name == \"ADWIN_ISO\":\n",
    "                drift = det.update(err_iso)\n",
    "            else:\n",
    "                drift = det.update(x_val)\n",
    "            if drift:\n",
    "                drift_detected = True\n",
    "                drift_points[name].append(i)\n",
    "\n",
    "        # Adaptation Latency & Recovery\n",
    "        if drift_detected and not in_recovery:\n",
    "            in_recovery = True\n",
    "            recovery_start = i\n",
    "\n",
    "        if in_recovery:\n",
    "            window_size = 500  # smoother window\n",
    "            if i - recovery_start > window_size:\n",
    "                recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled[i-window_size:i]]\n",
    "                rec_acc = 1 - np.mean(recent_errs)\n",
    "\n",
    "                if rec_acc > 0.90:  # recovered if accuracy >90%\n",
    "                    AL = i - recovery_start\n",
    "                    AL_values.append(AL)\n",
    "                    DDA_values.append(AL)\n",
    "                    RA_values.append(rec_acc * 100)  # absolute RA %\n",
    "                    in_recovery = False\n",
    "\n",
    "    # FAR = detections / total samples\n",
    "    total_samples = len(X_scaled)\n",
    "    for det, pts in drift_points.items():\n",
    "        FAR_values[det] = len(pts) / total_samples * 100\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n=== SUMMARY ===============================\")\n",
    "    print(f\"Data type: {data_type}\")\n",
    "    print(f\"Samples: {len(X_scaled)} | Features: {features[:10]} ...\")\n",
    "    print(\"------------------------------------------\")\n",
    "    for det, pts in drift_points.items():\n",
    "        print(f\"  {det:12}: {len(pts)} detections (FAR={FAR_values[det]:.2f}%)\")\n",
    "\n",
    "    if AL_values:\n",
    "        print(f\"\\nAdaptation Latency (AL): {np.mean(AL_values):.2f} samples\")\n",
    "        print(f\"Detection Delay Accuracy (DDA): {np.mean(DDA_values):.2f} samples\")\n",
    "        print(f\"Recovery Accuracy (RA): {np.mean(RA_values):.2f}%\")\n",
    "    else:\n",
    "        print(\"\\nNo AL/DDA/RA measured (no recovery detected).\")\n",
    "    print(\"==========================================\\n\")\n",
    "\n",
    "    return drift_points, AL_values, DDA_values, RA_values, FAR_values\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run for CMU keystroke dataset\n",
    "# -----------------------------\n",
    "datasets = {\n",
    "    \"keystroke\": \"/Users/festusedward-n/Documents/Datasets/DSL-StrongPasswordData 2.csv\"\n",
    "}\n",
    "\n",
    "for dtype, path in datasets.items():\n",
    "    drift_points, AL, DDA, RA, FAR = detect_natural_drift(path, dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f276f4d-f1a4-4c6d-8ead-ba1c450c2ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7ea2ce-add2-41d6-bd92-3dcc360d40e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7496e249-7f78-4f17-a7af-070189cd29b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keystroke dataset loaded: 20400 rows\n",
      "Available columns: ['subject', 'sessionIndex', 'rep', 'H.period', 'DD.period.t', 'UD.period.t', 'H.t', 'DD.t.i', 'UD.t.i', 'H.i', 'DD.i.e', 'UD.i.e', 'H.e', 'DD.e.five', 'UD.e.five', 'H.five', 'DD.five.Shift.r', 'UD.five.Shift.r', 'H.Shift.r', 'DD.Shift.r.o'] ...\n",
      "\n",
      "=== SUMMARY ===============================\n",
      "Data type: keystroke\n",
      "Samples: 20400 | Features: ['H.period', 'DD.period.t', 'UD.period.t', 'H.t', 'DD.t.i', 'UD.t.i', 'H.i', 'DD.i.e', 'UD.i.e', 'H.e'] ...\n",
      "------------------------------------------\n",
      "  ADWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  PH_PC       : 0 detections (FAR=0.00%)\n",
      "  KSWIN_PC    : 0 detections (FAR=0.00%)\n",
      "  DDM_ERR     : 17663 detections (FAR=86.58%)\n",
      "  PCDM_PC     : 605 detections (FAR=2.97%)\n",
      "  ADWIN_OCSVM : 0 detections (FAR=0.00%)\n",
      "  ADWIN_ISO   : 0 detections (FAR=0.00%)\n",
      "\n",
      "Adaptation Latency (AL): 2740.67 samples\n",
      "Detection Delay Accuracy (DDA): 2740.67 samples\n",
      "Recovery Accuracy (RA): 100.24%\n",
      "Average Error Rate (AER): 44.59%\n",
      "Drift Density (DD): 2.97%\n",
      "==========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from river.drift import ADWIN, PageHinkley, KSWIN\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# -----------------------------\n",
    "# Custom detectors\n",
    "# -----------------------------\n",
    "class DDM:\n",
    "    def __init__(self, min_num_instances=30, warning_level=2.0, drift_level=3.0):\n",
    "        self.min_num_instances = min_num_instances\n",
    "        self.warning_level = warning_level\n",
    "        self.drift_level = drift_level\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "        self.n = 0\n",
    "        self.drift_detected = False\n",
    "        self.mean_min = float('inf')\n",
    "        self.std_min = float('inf')\n",
    "\n",
    "    def add_element(self, error):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = error\n",
    "            self.std = 0.0\n",
    "        else:\n",
    "            old_mean = self.mean\n",
    "            self.mean += (error - old_mean) / self.n\n",
    "            self.std = np.sqrt(\n",
    "                (self.std**2 * (self.n - 1) + (error - self.mean) * (error - old_mean)) / self.n\n",
    "            )\n",
    "        if self.n >= self.min_num_instances:\n",
    "            if self.mean + self.std > self.mean_min + self.drift_level * self.std_min:\n",
    "                self.drift_detected = True\n",
    "            else:\n",
    "                self.drift_detected = False\n",
    "            if not self.drift_detected:\n",
    "                self.mean_min = min(self.mean_min, self.mean)\n",
    "                self.std_min = min(self.std_min, self.std)\n",
    "        return self.drift_detected\n",
    "\n",
    "\n",
    "class PCDM:\n",
    "    def __init__(self, window_size=50, n_permutations=50, alpha=0.05):\n",
    "        self.window_size = window_size\n",
    "        self.n_permutations = n_permutations\n",
    "        self.alpha = alpha\n",
    "        self.reference_window = []\n",
    "        self.current_window = []\n",
    "        self.drift_detected = False\n",
    "\n",
    "    def add_element(self, value):\n",
    "        self.current_window.append(value)\n",
    "        if len(self.current_window) > self.window_size:\n",
    "            self.current_window.pop(0)\n",
    "        if len(self.reference_window) < self.window_size:\n",
    "            self.reference_window.append(value)\n",
    "            return False\n",
    "        if len(self.current_window) == self.window_size:\n",
    "            stat, p_value = self._permutation_test()\n",
    "            self.drift_detected = p_value < self.alpha\n",
    "            if self.drift_detected:\n",
    "                self.reference_window = self.current_window.copy()\n",
    "        return self.drift_detected\n",
    "\n",
    "    def _permutation_test(self):\n",
    "        ref = np.array(self.reference_window)\n",
    "        curr = np.array(self.current_window)\n",
    "        observed_diff = np.abs(np.mean(ref) - np.mean(curr))\n",
    "        combined = np.concatenate([ref, curr])\n",
    "        perm_diffs = []\n",
    "        for _ in range(self.n_permutations):\n",
    "            np.random.shuffle(combined)\n",
    "            perm_ref = combined[: self.window_size]\n",
    "            perm_curr = combined[self.window_size :]\n",
    "            perm_diffs.append(np.abs(np.mean(perm_ref) - np.mean(perm_curr)))\n",
    "        p_value = np.sum(np.array(perm_diffs) >= observed_diff) / self.n_permutations\n",
    "        return observed_diff, p_value\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Drift detection + metrics pipeline\n",
    "# -----------------------------\n",
    "def detect_natural_drift(data_file, data_type):\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(f\"{data_type.capitalize()} dataset loaded: {len(df)} rows\")\n",
    "    print(f\"Available columns: {list(df.columns)[:20]} ...\")\n",
    "\n",
    "    # Select numeric features (exclude metadata)\n",
    "    exclude_cols = [\"subject\", \"sessionIndex\", \"rep\"]\n",
    "    features = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "    if not features:\n",
    "        raise ValueError(f\"No valid features found in {data_type} dataset!\")\n",
    "\n",
    "    X = df[features].dropna().values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Base anomaly models\n",
    "    ocsvm = OneClassSVM(nu=0.01, gamma=\"scale\")\n",
    "    iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "    ocsvm.fit(X_scaled[:1000])\n",
    "    iso.fit(X_scaled[:1000])\n",
    "\n",
    "    # Initialize detectors\n",
    "    detectors = {\n",
    "        \"ADWIN_PC\": ADWIN(delta=0.01),\n",
    "        \"PH_PC\": PageHinkley(threshold=10, alpha=0.01),\n",
    "        \"KSWIN_PC\": KSWIN(alpha=0.1, window_size=100),\n",
    "        \"DDM_ERR\": DDM(),\n",
    "        \"PCDM_PC\": PCDM(alpha=0.05),\n",
    "        \"ADWIN_OCSVM\": ADWIN(delta=0.01),\n",
    "        \"ADWIN_ISO\": ADWIN(delta=0.01),\n",
    "    }\n",
    "\n",
    "    drift_points = {name: [] for name in detectors}\n",
    "    AL_values, DDA_values, RA_values = [], [], []\n",
    "    FAR_values = {}\n",
    "\n",
    "    in_recovery = False\n",
    "    recovery_start = None\n",
    "    baseline_acc = None\n",
    "    error_rates = []\n",
    "\n",
    "    for i, x in enumerate(X_scaled):\n",
    "        x_val = np.mean(x)\n",
    "        err_ocsvm = 1 if ocsvm.predict([x])[0] == -1 else 0\n",
    "        err_iso = 1 if iso.predict([x])[0] == -1 else 0\n",
    "        error_rates.append(err_ocsvm or err_iso)\n",
    "\n",
    "        drift_detected = False\n",
    "        for name, det in detectors.items():\n",
    "            if name == \"DDM_ERR\":\n",
    "                drift = det.add_element(err_ocsvm or err_iso)\n",
    "            elif name == \"PCDM_PC\":\n",
    "                drift = det.add_element(x_val)\n",
    "            elif name == \"ADWIN_OCSVM\":\n",
    "                drift = det.update(err_ocsvm)\n",
    "            elif name == \"ADWIN_ISO\":\n",
    "                drift = det.update(err_iso)\n",
    "            else:\n",
    "                drift = det.update(x_val)\n",
    "            if drift:\n",
    "                drift_detected = True\n",
    "                drift_points[name].append(i)\n",
    "\n",
    "        # Adaptation Latency\n",
    "        if drift_detected and not in_recovery:\n",
    "            in_recovery = True\n",
    "            recovery_start = i\n",
    "            recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled[max(0, i-200):i]]\n",
    "            baseline_acc = 1 - np.mean(recent_errs) if len(recent_errs) > 0 else None\n",
    "\n",
    "        if in_recovery:\n",
    "            if i - recovery_start > 200:\n",
    "                recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled[i-200:i]]\n",
    "                if np.mean(recent_errs) < 0.05:  # recovered\n",
    "                    AL = i - recovery_start\n",
    "                    AL_values.append(AL)\n",
    "                    DDA_values.append(AL)\n",
    "                    if baseline_acc is not None:\n",
    "                        rec_acc = 1 - np.mean(recent_errs)\n",
    "                        RA_values.append(rec_acc / max(baseline_acc, 1e-6) * 100)\n",
    "                    in_recovery = False\n",
    "\n",
    "    # FAR = detections / total samples\n",
    "    total_samples = len(X_scaled)\n",
    "    for det, pts in drift_points.items():\n",
    "        FAR_values[det] = len(pts) / total_samples * 100\n",
    "\n",
    "    # AER = mean error rate\n",
    "    AER = np.mean(error_rates) * 100\n",
    "\n",
    "    # Drift Density (DD) = (#drift detections by PCDM) / total_samples * 100\n",
    "    DD = len(drift_points[\"PCDM_PC\"]) / total_samples * 100\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n=== SUMMARY ===============================\")\n",
    "    print(f\"Data type: {data_type}\")\n",
    "    print(f\"Samples: {total_samples} | Features: {features[:10]} ...\")\n",
    "    print(\"------------------------------------------\")\n",
    "    for det, pts in drift_points.items():\n",
    "        print(f\"  {det:12}: {len(pts)} detections (FAR={FAR_values[det]:.2f}%)\")\n",
    "    if AL_values:\n",
    "        print(f\"\\nAdaptation Latency (AL): {np.mean(AL_values):.2f} samples\")\n",
    "        print(f\"Detection Delay Accuracy (DDA): {np.mean(DDA_values):.2f} samples\")\n",
    "        print(f\"Recovery Accuracy (RA): {np.mean(RA_values):.2f}%\")\n",
    "        print(f\"Average Error Rate (AER): {AER:.2f}%\")\n",
    "        print(f\"Drift Density (DD): {DD:.2f}%\")\n",
    "    else:\n",
    "        print(\"\\nNo AL/DDA/RA measured (no recovery detected).\")\n",
    "    print(\"==========================================\\n\")\n",
    "\n",
    "    return drift_points, AL_values, DDA_values, RA_values, FAR_values, AER, DD\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run for CMU keystroke dataset\n",
    "# -----------------------------\n",
    "datasets = {\n",
    "    \"keystroke\": \"/Users/festusedward-n/Documents/Datasets/DSL-StrongPasswordData 2.csv\"\n",
    "}\n",
    "\n",
    "for dtype, path in datasets.items():\n",
    "    detect_natural_drift(path, dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a53513-965a-4d2e-baf1-010b0d005b78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env_310)",
   "language": "python",
   "name": "pytorch_env_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
