{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "869c339b-a850-4292-b8f9-4802f8647368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing User1 ===\n",
      "\n",
      "=== Processing User10 ===\n",
      "\n",
      "=== Processing User11 ===\n",
      "\n",
      "=== Processing User12 ===\n",
      "\n",
      "=== Processing User13 ===\n",
      "\n",
      "=== Processing User14 ===\n",
      "\n",
      "=== Processing User15 ===\n",
      "\n",
      "=== Processing User16 ===\n",
      "\n",
      "=== Processing User17 ===\n",
      "\n",
      "=== Processing User18 ===\n",
      "\n",
      "=== Processing User19 ===\n",
      "\n",
      "=== Processing User2 ===\n",
      "❌ Error in /Users/festusedward-n/Documents/Datasets/DFL_Mouse_Dynamics_Dataset/Users/User2/2018_11_11__18_32_58.CSV: Error tokenizing data. C error: Expected 5 fields in line 265, saw 6\n",
      "\n",
      "\n",
      "=== Processing User20 ===\n",
      "\n",
      "=== Processing User21 ===\n",
      "\n",
      "=== Processing User3 ===\n",
      "\n",
      "=== Processing User4 ===\n",
      "\n",
      "=== Processing User5 ===\n",
      "\n",
      "=== Processing User6 ===\n",
      "\n",
      "=== Processing User7 ===\n",
      "\n",
      "=== Processing User8 ===\n",
      "\n",
      "=== Processing User9 ===\n",
      "\n",
      "=== AGGREGATE SUMMARY ACROSS USERS ===\n",
      "      user  files  detections      AL_mean\n",
      "0    User1     31      983299   296.963220\n",
      "1   User10      1           0          NaN\n",
      "2   User11     46     1543683   368.131144\n",
      "3   User12     17     3194083   283.215500\n",
      "4   User13     19      844754   398.357279\n",
      "5   User14     20      926842   355.293230\n",
      "6   User15     52     4905656   400.462034\n",
      "7   User16      6      712049   244.031347\n",
      "8   User17     18     1038692   374.440463\n",
      "9   User18     58    12132778   474.731505\n",
      "10  User19      6    10357048   235.937141\n",
      "11   User2     20      487699   382.580016\n",
      "12  User20     30      345286   255.837313\n",
      "13  User21     24      579850   323.978102\n",
      "14   User3     29      377370   694.832753\n",
      "15   User4      3    41354752   269.088790\n",
      "16   User5      7     1931549   281.128486\n",
      "17   User6      2     1781073   297.241333\n",
      "18   User7      3      531412   272.374935\n",
      "19   User8      8      326314   252.316446\n",
      "20   User9      1      460308  4738.278351\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from river.drift import ADWIN, PageHinkley, KSWIN\n",
    "\n",
    "# -----------------------------\n",
    "# Feature Extraction\n",
    "# -----------------------------\n",
    "def extract_mouse_features(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure column names are clean\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    required = {\"client timestamp\", \"x\", \"y\"}\n",
    "    if not required.issubset(df.columns):\n",
    "        raise ValueError(f\"Missing required columns in {file_path}\")\n",
    "\n",
    "    # Compute deltas\n",
    "    df[\"dx\"] = df[\"x\"].diff().fillna(0)\n",
    "    df[\"dy\"] = df[\"y\"].diff().fillna(0)\n",
    "    df[\"dt\"] = df[\"client timestamp\"].diff().replace(0, np.nan).fillna(1)\n",
    "\n",
    "    # Features\n",
    "    df[\"distance\"] = np.sqrt(df[\"dx\"]**2 + df[\"dy\"]**2)\n",
    "    df[\"speed\"] = df[\"distance\"] / df[\"dt\"]\n",
    "\n",
    "    return df[[\"speed\", \"distance\", \"dx\", \"dy\"]].fillna(0)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Custom Drift Detectors\n",
    "# -----------------------------\n",
    "class DDM:\n",
    "    def __init__(self, min_num_instances=30, warning_level=2.0, drift_level=3.0):\n",
    "        self.min_num_instances = min_num_instances\n",
    "        self.warning_level = warning_level\n",
    "        self.drift_level = drift_level\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "        self.n = 0\n",
    "        self.drift_detected = False\n",
    "        self.mean_min = float(\"inf\")\n",
    "        self.std_min = float(\"inf\")\n",
    "\n",
    "    def add_element(self, error):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = error\n",
    "            self.std = 0.0\n",
    "        else:\n",
    "            old_mean = self.mean\n",
    "            self.mean += (error - old_mean) / self.n\n",
    "            self.std = np.sqrt(\n",
    "                (self.std**2 * (self.n - 1) + (error - self.mean) * (error - old_mean)) / self.n\n",
    "            )\n",
    "\n",
    "        if self.n >= self.min_num_instances:\n",
    "            if self.mean + self.std > self.mean_min + self.drift_level * self.std_min:\n",
    "                self.drift_detected = True\n",
    "            else:\n",
    "                self.drift_detected = False\n",
    "            if not self.drift_detected:\n",
    "                self.mean_min = min(self.mean_min, self.mean)\n",
    "                self.std_min = min(self.std_min, self.std)\n",
    "        return self.drift_detected\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Drift Detection on Features\n",
    "# -----------------------------\n",
    "def detect_natural_drift_from_features(features):\n",
    "    X = features.values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Base models\n",
    "    ocsvm = OneClassSVM(nu=0.01, gamma=\"scale\")\n",
    "    iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "    ocsvm.fit(X_scaled[:min(1000, len(X_scaled))])\n",
    "    iso.fit(X_scaled[:min(1000, len(X_scaled))])\n",
    "\n",
    "    # Initialize detectors\n",
    "    detectors = {\n",
    "        \"ADWIN_PC\": ADWIN(delta=0.01),\n",
    "        \"PH_PC\": PageHinkley(threshold=10, alpha=0.01),\n",
    "        \"KSWIN_PC\": KSWIN(alpha=0.1, window_size=100),\n",
    "        \"DDM_ERR\": DDM(),\n",
    "        \"ADWIN_OCSVM\": ADWIN(delta=0.01),\n",
    "        \"ADWIN_ISO\": ADWIN(delta=0.01),\n",
    "    }\n",
    "\n",
    "    drift_points = {name: [] for name in detectors}\n",
    "    AL_values = []\n",
    "    in_recovery, recovery_start = False, None\n",
    "\n",
    "    for i, x in enumerate(X_scaled):\n",
    "        x_val = np.mean(x)\n",
    "        err_ocsvm = 1 if ocsvm.predict([x])[0] == -1 else 0\n",
    "        err_iso = 1 if iso.predict([x])[0] == -1 else 0\n",
    "\n",
    "        drift_detected = False\n",
    "        for name, det in detectors.items():\n",
    "            if name == \"DDM_ERR\":\n",
    "                drift = det.add_element(err_ocsvm or err_iso)\n",
    "            elif name == \"ADWIN_OCSVM\":\n",
    "                drift = det.update(err_ocsvm)\n",
    "            elif name == \"ADWIN_ISO\":\n",
    "                drift = det.update(err_iso)\n",
    "            else:\n",
    "                drift = det.update(x_val)\n",
    "            if drift:\n",
    "                drift_detected = True\n",
    "                drift_points[name].append(i)\n",
    "\n",
    "        # Adaptation Latency (example: using OCSVM recovery)\n",
    "        if drift_detected and not in_recovery:\n",
    "            in_recovery, recovery_start = True, i\n",
    "\n",
    "        if in_recovery and i - recovery_start > 200:\n",
    "            recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled[i-200:i]]\n",
    "            if np.mean(recent_errs) < 0.05:\n",
    "                AL = i - recovery_start\n",
    "                AL_values.append(AL)\n",
    "                in_recovery = False\n",
    "\n",
    "    return drift_points, AL_values\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main Loop Over Users\n",
    "# -----------------------------\n",
    "root = \"/Users/festusedward-n/Documents/Datasets/DFL_Mouse_Dynamics_Dataset/Users\"\n",
    "summary = []\n",
    "\n",
    "for user in sorted(os.listdir(root)):\n",
    "    user_path = os.path.join(root, user)\n",
    "    if not os.path.isdir(user_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== Processing {user} ===\")\n",
    "    user_results = {\"user\": user, \"files\": 0, \"detections\": 0, \"AL_mean\": None}\n",
    "\n",
    "    drift_counts, all_AL = [], []\n",
    "\n",
    "    for file in os.listdir(user_path):\n",
    "        if not file.endswith(\".CSV\"):\n",
    "            continue\n",
    "        file_path = os.path.join(user_path, file)\n",
    "\n",
    "        try:\n",
    "            feats = extract_mouse_features(file_path)\n",
    "            if feats.empty:\n",
    "                print(f\"⚠ Skipped {file_path} (no usable data)\")\n",
    "                continue\n",
    "\n",
    "            drift_points, AL = detect_natural_drift_from_features(feats)\n",
    "            user_results[\"files\"] += 1\n",
    "            user_results[\"detections\"] += sum(len(v) for v in drift_points.values())\n",
    "            if AL:\n",
    "                all_AL.extend(AL)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {file_path}: {e}\")\n",
    "\n",
    "    if all_AL:\n",
    "        user_results[\"AL_mean\"] = np.mean(all_AL)\n",
    "\n",
    "    summary.append(user_results)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Aggregate Summary\n",
    "# -----------------------------\n",
    "print(\"\\n=== AGGREGATE SUMMARY ACROSS USERS ===\")\n",
    "df_summary = pd.DataFrame(summary)\n",
    "print(df_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e3c3d1-dcf7-44ac-b4f6-85a0d2072e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475cdb5f-09fe-42d7-8058-e1e1b79988a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "301c289c-1978-469a-89c6-cf99ee012bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing User1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-8:\n",
      "Process SpawnProcess-5:\n",
      "Process SpawnProcess-4:\n",
      "Process SpawnProcess-1:\n",
      "Process SpawnProcess-6:\n",
      "Process SpawnProcess-3:\n",
      "Process SpawnProcess-7:\n",
      "Process SpawnProcess-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "Traceback (most recent call last):\n",
      "AttributeError: Can't get attribute 'process_file' on <module '__main__' (built-in)>\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'process_file' on <module '__main__' (built-in)>\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'process_file' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'process_file' on <module '__main__' (built-in)>\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'process_file' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'process_file' on <module '__main__' (built-in)>\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'process_file' on <module '__main__' (built-in)>\n",
      "AttributeError: Can't get attribute 'process_file' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 156\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(as_completed(futures), \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    155\u001b[0m     file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(futures[future])\n\u001b[0;32m--> 156\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   → [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch_env_310/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from river.drift import ADWIN, PageHinkley, KSWIN\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# -----------------------------\n",
    "# Feature Extraction\n",
    "# -----------------------------\n",
    "def extract_mouse_features(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    required = {\"client timestamp\", \"x\", \"y\"}\n",
    "    if not required.issubset(df.columns):\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df[\"dx\"] = df[\"x\"].diff().fillna(0)\n",
    "    df[\"dy\"] = df[\"y\"].diff().fillna(0)\n",
    "    df[\"dt\"] = df[\"client timestamp\"].diff().replace(0, np.nan).fillna(1)\n",
    "\n",
    "    df[\"distance\"] = np.sqrt(df[\"dx\"]**2 + df[\"dy\"]**2)\n",
    "    df[\"speed\"] = df[\"distance\"] / df[\"dt\"]\n",
    "\n",
    "    return df[[\"speed\", \"distance\", \"dx\", \"dy\"]].fillna(0)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Custom DDM\n",
    "# -----------------------------\n",
    "class DDM:\n",
    "    def __init__(self, min_num_instances=30, drift_level=3.0):\n",
    "        self.min_num_instances = min_num_instances\n",
    "        self.drift_level = drift_level\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "        self.n = 0\n",
    "        self.drift_detected = False\n",
    "        self.mean_min = float(\"inf\")\n",
    "        self.std_min = float(\"inf\")\n",
    "\n",
    "    def add_element(self, error):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = error\n",
    "        else:\n",
    "            old_mean = self.mean\n",
    "            self.mean += (error - old_mean) / self.n\n",
    "            self.std = np.sqrt(\n",
    "                (self.std**2 * (self.n - 1) + (error - self.mean) * (error - old_mean)) / self.n\n",
    "            )\n",
    "\n",
    "        if self.n >= self.min_num_instances:\n",
    "            if self.mean + self.std > self.mean_min + self.drift_level * self.std_min:\n",
    "                self.drift_detected = True\n",
    "            else:\n",
    "                self.drift_detected = False\n",
    "            if not self.drift_detected:\n",
    "                self.mean_min = min(self.mean_min, self.mean)\n",
    "                self.std_min = min(self.std_min, self.std)\n",
    "        return self.drift_detected\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Drift Detection per File\n",
    "# -----------------------------\n",
    "def process_file(file_path):\n",
    "    try:\n",
    "        feats = extract_mouse_features(file_path)\n",
    "        if feats.empty:\n",
    "            return {\"detections\": 0, \"AL\": []}\n",
    "\n",
    "        X = feats.values\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        ocsvm = OneClassSVM(nu=0.01, gamma=\"scale\")\n",
    "        iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "        train_size = min(500, len(X_scaled))\n",
    "        ocsvm.fit(X_scaled[:train_size])\n",
    "        iso.fit(X_scaled[:train_size])\n",
    "\n",
    "        detectors = {\n",
    "            \"ADWIN_PC\": ADWIN(delta=0.01),\n",
    "            \"PH_PC\": PageHinkley(threshold=10, alpha=0.01),\n",
    "            \"KSWIN_PC\": KSWIN(alpha=0.1, window_size=100),\n",
    "            \"DDM_ERR\": DDM(),\n",
    "            \"ADWIN_OCSVM\": ADWIN(delta=0.01),\n",
    "            \"ADWIN_ISO\": ADWIN(delta=0.01),\n",
    "        }\n",
    "\n",
    "        drift_points = {name: [] for name in detectors}\n",
    "        AL_values = []\n",
    "        in_recovery, recovery_start = False, None\n",
    "\n",
    "        for i, x in enumerate(X_scaled):\n",
    "            x_val = np.mean(x)\n",
    "            err_ocsvm = 1 if ocsvm.predict([x])[0] == -1 else 0\n",
    "            err_iso = 1 if iso.predict([x])[0] == -1 else 0\n",
    "\n",
    "            drift_detected = False\n",
    "            for name, det in detectors.items():\n",
    "                if name == \"DDM_ERR\":\n",
    "                    drift = det.add_element(err_ocsvm or err_iso)\n",
    "                elif name == \"ADWIN_OCSVM\":\n",
    "                    drift = det.update(err_ocsvm)\n",
    "                elif name == \"ADWIN_ISO\":\n",
    "                    drift = det.update(err_iso)\n",
    "                else:\n",
    "                    drift = det.update(x_val)\n",
    "                if drift:\n",
    "                    drift_detected = True\n",
    "                    drift_points[name].append(i)\n",
    "\n",
    "            if drift_detected and not in_recovery:\n",
    "                in_recovery, recovery_start = True, i\n",
    "            if in_recovery and i - recovery_start > 200:\n",
    "                recent_errs = [1 if ocsvm.predict([z])[0] == -1 else 0 for z in X_scaled[i-200:i]]\n",
    "                if np.mean(recent_errs) < 0.05:\n",
    "                    AL_values.append(i - recovery_start)\n",
    "                    in_recovery = False\n",
    "\n",
    "        return {\"detections\": sum(len(v) for v in drift_points.values()), \"AL\": AL_values}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"detections\": 0, \"AL\": [], \"error\": str(e)}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main Parallel Loop Over Users\n",
    "# -----------------------------\n",
    "root = \"/Users/festusedward-n/Documents/Datasets/DFL_Mouse_Dynamics_Dataset/Users\"\n",
    "summary = []\n",
    "MAX_FILES = 10  # limit per user for testing\n",
    "\n",
    "for user in sorted(os.listdir(root)):\n",
    "    user_path = os.path.join(root, user)\n",
    "    if not os.path.isdir(user_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== Processing {user} ===\")\n",
    "    user_results = {\"user\": user, \"files\": 0, \"detections\": 0, \"AL_mean\": None}\n",
    "    all_AL = []\n",
    "\n",
    "    files = [os.path.join(user_path, f) for f in os.listdir(user_path) if f.endswith(\".CSV\")]\n",
    "    if MAX_FILES:\n",
    "        files = files[:MAX_FILES]\n",
    "\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_file, f): f for f in files}\n",
    "        for idx, future in enumerate(as_completed(futures), 1):\n",
    "            file = os.path.basename(futures[future])\n",
    "            result = future.result()\n",
    "            print(f\"   → [{idx}/{len(files)}] {file}\")\n",
    "\n",
    "            if \"error\" in result:\n",
    "                print(f\"     ❌ Error: {result['error']}\")\n",
    "            else:\n",
    "                user_results[\"files\"] += 1\n",
    "                user_results[\"detections\"] += result[\"detections\"]\n",
    "                all_AL.extend(result[\"AL\"])\n",
    "\n",
    "    if all_AL:\n",
    "        user_results[\"AL_mean\"] = np.mean(all_AL)\n",
    "    summary.append(user_results)\n",
    "    print(f\"--- {user} finished: {user_results['files']} files, {user_results['detections']} detections, AL={user_results['AL_mean']}\")\n",
    "\n",
    "print(\"\\n=== AGGREGATE SUMMARY ACROSS USERS ===\")\n",
    "df_summary = pd.DataFrame(summary)\n",
    "print(df_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a228585-8ffe-49e5-a403-8e0a89fcf90b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env_310)",
   "language": "python",
   "name": "pytorch_env_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
